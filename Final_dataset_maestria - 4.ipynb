{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos todas las bibliotecas necesarias para ejecutar todos los métodos del jupyter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es buena costumbre cargarlos al principio, asi no se estar cargando en memoria repetidas veces cada paquete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "#import pydotplus\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(df, column):\n",
    "    q1 = df_final[column].quantile(.25)\n",
    "    q2 = df_final[column].quantile(.5)\n",
    "    q3 = df_final[column].quantile(.75)\n",
    "    df[column+\"_in_q1\"] = 0\n",
    "    df[column+\"_in_q2\"] = 0\n",
    "    df[column+\"_in_q3\"] = 0\n",
    "    df[column+\"_in_q4\"] = 0\n",
    "    df[column+\"_in_q1\"] = list(map(lambda x: 1 if x <=q1 else 0, df[column]))\n",
    "    df[column+\"_in_q2\"] = list(map(lambda x: 1 if (q1 < x and x <=q2) else 0, df[column]))\n",
    "    df[column+\"_in_q3\"] = list(map(lambda x: 1 if (q2 < x and x <=q3) else 0, df[column]))\n",
    "    df[column+\"_in_q4\"] = list(map(lambda x: 1 if x>q3 else 0, df[column]))\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_object(df,column,object_name):\n",
    "    df[\"is_\"+str(object_name)] = list(map(lambda x: 1 if x == object_name else 0, df[column]))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_replace_nans(df,col,rate_to_complete):\n",
    "    \n",
    "    index_list = df[col][df[col].isnull()].sample(frac=rate_to_complete,random_state=1).index.tolist()\n",
    "    \n",
    "    if len(index_list)>0:\n",
    "        moda = df[(~df[col].isnull())][col].mode()[0]\n",
    "        df.loc[index_list,col] = moda\n",
    "        print(\"Completando columna \" + str(col))\n",
    "        print(\"Moda: \" + str(moda))\n",
    "        print(\"Cantidad restante de NULLs en columna \" + str(col) + \": \"+str(df[col][df[col].isnull()].shape))\n",
    "        \n",
    "    return df\n",
    "    \n",
    "def complete_nans(df,null_cols,rate_to_complete,method):\n",
    "    if method==\"moda_por_clase\":\n",
    "        for col in null_cols:\n",
    "            print(\"Completando clase 0\")\n",
    "            df0 = find_and_replace_nans(df[df.target==0.0],col,rate_to_complete)\n",
    "            print(\"Completando clase 1\")\n",
    "            df1 = find_and_replace_nans(df[df.target==1.0],col,rate_to_complete)\n",
    "            df = pd.concat([df0,df1])\n",
    "    else:\n",
    "        for col in null_cols:\n",
    "            df = find_and_replace_nans(df,col,rate_to_complete)\n",
    "    \n",
    "    return df.dropna()\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, metodo, rate_to_complete):\n",
    "    \n",
    "    # Extraemos las filas de la columna 'model' que tienen valores NaNs y luego los completamos con un valor por defecto\n",
    "    \n",
    "    #model_null_indexes = df[df.model.isnull()].index.tolist()\n",
    "    #df.loc[model_null_indexes,\"model\"] = \"other other\"\n",
    "    \n",
    "    df['model'] = df['model'].fillna('other other')\n",
    "    \n",
    "    # Obtenemos las columnas que tienen al menos un NaN\n",
    "    cols_in_null = df.columns[df.isnull().sum(axis=0)>0].tolist()\n",
    "    \n",
    "    # Los completamos con un rate_to_complete y utilizando un determinado metodo\n",
    "    df_comp = complete_nans(df,cols_in_null,rate_to_complete,metodo)\n",
    "    \n",
    "    # Discretizamos las columnas que ya sabemos que tienen una distribucion dada (Chi, Normal)\n",
    "    # (Se usan cuantiles para ver en qué segmento caen)\n",
    "    for column in [\"diff_time_days_minnone\",\"diff_time_dayssearch engine hit\",\"COUNT(users_logs)\",\"number_visits_model\",\"diff_time_daysbrand listing\",\"diff_time_daysnone\"]:\n",
    "        df_comp = discretize(df_comp,column)\n",
    "    \n",
    "    # De la columna 'model' extraemos marca y modelo del celular visitado o  comprado\n",
    "    df_comp[\"marca\"] = df_comp.model.str.split(\" \",1,expand=True)[0]\n",
    "    df_comp[\"modelo\"] = df_comp.model.str.split(\" \",1,expand=True)[1]\n",
    "    \n",
    "    # Determinamos de que marca se trata\n",
    "    df_comp[\"is_iphone\"] = list(map(lambda x: 1 if x==\"iPhone\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_samsung\"] = list(map(lambda x: 1 if x==\"Samsung\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_motorola\"] = list(map(lambda x: 1 if x==\"Motorola\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_lg\"] = list(map(lambda x: 1 if x==\"LG\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_sony\"] = list(map(lambda x: 1 if x==\"Sony\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_other\"] = list(map(lambda x: 1 if (x!=\"Sony\" and x!=\"LG\" and x!=\"Motorola\" and x!=\"Samsung\" and x!=\"iPhone\") else 0, df_comp[\"marca\"]))\n",
    "    \n",
    "    # determinamos de que modelo se trata\n",
    "    pop_models = ['6','5s','6S','Galaxy J5','7','7 Plus','Galaxy S8','Galaxy S7','Galaxy S7 Edge','Galaxy J7 Prime','Moto G2 3G Dual','Galaxy S6 Edge','Galaxy S6 Flat','5c','6S Plus','Galaxy J7','6 Plus','Moto G4 Plus','SE','Galaxy Gran Prime Duos TV','4S','Galaxy S8 Plus','5','Moto G3 4G','Galaxy A5 2017','Moto X Play 4G Dual','Moto G5 Plus','Galaxy A7 2017','Moto X2','Galaxy A5']\n",
    "    for model in pop_models:\n",
    "        df_comp = is_object(df_comp,\"modelo\",model)\n",
    "\n",
    "    df_comp[\"is_other_model\"] = list(map(lambda x: 1 if x not in pop_models else 0, df_comp[\"modelo\"]))\n",
    "    \n",
    "    # Eliminamos las columnas con strings\n",
    "    df_comp.drop([\"model\",\"marca\",\"modelo\"],axis=1,inplace=True)\n",
    "    \n",
    "    return df_comp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos los datasets para construir el dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first = pd.read_csv('../../data_aa/first_part_dataset.csv')\n",
    "\n",
    "df_second = pd.read_csv('../../data_aa/second_part_dataset.csv')\n",
    "\n",
    "df_thrid = pd.read_csv('../../data_aa/thrid_part_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first = df_first.rename(columns={'person_id': 'person'})\n",
    "\n",
    "df_first = df_first.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'person', 'diff_time_days_minnone',\n",
       "       'diff_time_daysnone', 'diff_time_dayscheckout',\n",
       "       'diff_time_daysconversion', 'diff_time_daysbrand listing',\n",
       "       'diff_time_dayssearch engine hit', 'diff_time_start_end', 'model',\n",
       "       'number_visits_model'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_second.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second = df_second.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'person', 'COUNT(users_logs)', 'avg_time_events'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_thrid.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thrid = df_thrid.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_first, df_second, on='person', how='left')\n",
    "\n",
    "df_final = pd.merge(df_final, df_thrid, on='person', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19414,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.target.isin([1.0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significado de las columnas:\n",
    "\n",
    " - Las columnas 'Android', 'BlackBerry', 'Chrome', 'FreeBSD', 'Linux', 'Mac', 'Tizen', 'Ubuntu', 'Unknown', 'Windows','iOS' indican la cantidad de veces que cada usuario ingreso a la pagina de Trocafone utilizando uno de esos esos sistemas operativos. Por ejemplo, si el valor de 'Android' para un usuario es 2 siginfica que ese usuario entro a la pagina dos veces desde un dispositivo con un sistema operativo Android\n",
    " \n",
    " - Las columnas 'Computer', 'Smartphone', 'Tablet' indican la cantidad de veces que cada usuario ingreso a la pagina de Trocafone utilizando uno de esos dispositivos\n",
    " \n",
    " - Las columnas 'ad campaign hit', 'brand listing', 'checkout', 'conversion', 'generic listing', 'lead','search engine hit', 'searched products', 'staticpage','viewed product', 'visited site' indican la cantidad de veces que cada usuario realizo cada uno de esos eventos\n",
    " \n",
    " - La columna 'diff_time_days_minnone' indica la cantidad de tiempo que paso desde el primer evento que realizo el usuario hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_daysnone' indica la cantidad de tiempo que paso desde el ultimo evento que el usuario realizo en la pagina hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_dayscheckout' indica la cantidad de tiempo que paso desde que el usuario realizo el ultimo checkout hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_daysconversion' indica la cantidad de tiempo que paso desde que el usuario realizo la ultima conversion hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_daysbrand listing' indica la cantidad de tiempo que paso desde que el usuario realizo la ultima brand listing hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_dayssearch engine hit' indica la cantidad de tiempo que paso desde que el usuario realizo el ultimo engine hit hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_start_end' indica la cantidad de tiempo que paso entre el primer evento que realizo el usuario en la pagina y el ultimo\n",
    " \n",
    " - La columna 'model' indica el modelo de celuar que el usuario mas veces visito\n",
    " \n",
    " - La columna 'number_visits_model' indica cuantas veces el usuario visito el celular que mas visito\n",
    " \n",
    " - La columna 'COUNT(users_logs)' indica la cantidad de eventos que el usuario realizo en la pagina\n",
    " \n",
    " - La columna 'avg_time_events' indica el tiempo promedio que paso entre los eventos realizados por cada usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminamos las columnas con muy pocas variables o alta corrleación (Ej. Windows ~ Computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop([\"BlackBerry\",\"Chrome\",\"FreeBSD\",\"Linux\",\"Mac\",\"Other\",\"Tizen\",\"Ubuntu\",\"Unknown\",\"Windows\",\"person\"],\n",
    "              axis=1,\n",
    "              inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformamos algunas columnas para que su distribución se ajuste a una Chi cuadrado o Normal. Luego discretizamos por cuantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.loc[:,\"diff_time_daysnone\"] = np.log10(df_final[\"diff_time_daysnone\"]+1.)\n",
    "df_final.loc[:,\"diff_time_days_minnone\"] = np.log10(df_final.diff_time_days_minnone+1.)\n",
    "df_final.loc[:,\"diff_time_dayssearch engine hit\"] = np.log10(df_final[\"diff_time_dayssearch engine hit\"]+1.0)\n",
    "df_final.loc[:,\"COUNT(users_logs)\"] = np.log10(df_final[\"COUNT(users_logs)\"]+1.)\n",
    "df_final.loc[:,\"number_visits_model\"] = np.log10(1.+df_final.number_visits_model)\n",
    "df_final.loc[:,\"diff_time_daysbrand listing\"] = np.log10(01.+df_final[\"diff_time_daysbrand listing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         NaN\n",
       "1                    iPhone 7\n",
       "2                   iPhone 6S\n",
       "3    Motorola Moto G2 3G Dual\n",
       "4      Samsung Galaxy S8 Plus\n",
       "Name: model, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['model'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En el siguiente metodo  se realiza todo el preprocess de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completando columna diff_time_dayscheckout\n",
      "Moda: 2.2\n",
      "Cantidad restante de NULLs en columna diff_time_dayscheckout: (0,)\n",
      "Completando columna diff_time_daysconversion\n",
      "Moda: 1.2\n",
      "Cantidad restante de NULLs en columna diff_time_daysconversion: (0,)\n",
      "Completando columna diff_time_daysbrand listing\n",
      "Moda: 0.11394335230683679\n",
      "Cantidad restante de NULLs en columna diff_time_daysbrand listing: (0,)\n",
      "Completando columna diff_time_dayssearch engine hit\n",
      "Moda: 0.11394335230683679\n",
      "Cantidad restante de NULLs en columna diff_time_dayssearch engine hit: (0,)\n",
      "Completando columna number_visits_model\n",
      "Moda: 0.3010299956639812\n",
      "Cantidad restante de NULLs en columna number_visits_model: (0,)\n",
      "Completando columna avg_time_events\n",
      "Moda: 0.0\n",
      "Cantidad restante de NULLs en columna avg_time_events: (0,)\n"
     ]
    }
   ],
   "source": [
    "df_completo = preprocess(df_final.copy(),\"moda\",1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a seleccionar solo 980 casos que no realizaron una compra para balancear el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    }
   ],
   "source": [
    "df_completo_reduced_1 = df_completo[df_completo['target']==1]\n",
    "print(len(df_completo_reduced_1.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    }
   ],
   "source": [
    "df_completo_reduced_0 = df_completo[df_completo['target']==0].sample(n=980, random_state=1)\n",
    "print(len(df_completo_reduced_0.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_completo_reduced_1, df_completo_reduced_0])\n",
    "del df_completo_reduced_0, df_completo_reduced_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nos quedamos con aquellas columnas que no sean target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = list(set(df_train.columns.tolist())-set([\"target\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenemos el dataset de test y lo eliminamos del dataset de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_train.sample(frac=0.2, random_state=2)\n",
    "df_train.drop(df_test.index,axis=0,inplace=True)\n",
    "train_indexes = df_train.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificamos el balance de categorías en cada datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = df_train.shape[0]*1.\n",
    "N_test = df_test.shape[0]*1.\n",
    "\n",
    "N_train_1 = df_train.target.sum()\n",
    "N_train_0 = df_train.shape[0]*1.-N_train_1\n",
    "\n",
    "N_test_1 = df_test.target.sum()\n",
    "N_test_0 = df_test.shape[0]*1.-N_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de casos positivos en el train: %50.255102040816325\n",
      "Porcentaje de casos positivos en el test: %48.97959183673469\n"
     ]
    }
   ],
   "source": [
    "print(\"Porcentaje de casos positivos en el train: %\" + str(100.0*N_train_1/N_train))\n",
    "print(\"Porcentaje de casos positivos en el test: %\" + str(100.0*N_test_1/N_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando únicamente el dataset de train ejecutamos 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy','roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(max_depth=3, criterion='gini') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n",
    "                        df_train[feature_columns].values, # X_train features\n",
    "                        df_train[\"target\"].values, # Y_train targets\n",
    "                        scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                        return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                        return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                        cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                        n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los resultados en un DF de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5_fold_resultados = pd.DataFrame(scores)\n",
    "cv_5_fold_resultados.drop([\"fit_time\",\"score_time\",\"estimator\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.767145</td>\n",
       "      <td>0.791728</td>\n",
       "      <td>0.828612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.767516</td>\n",
       "      <td>0.763955</td>\n",
       "      <td>0.836356</td>\n",
       "      <td>0.824302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.779904</td>\n",
       "      <td>0.753185</td>\n",
       "      <td>0.842019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.760383</td>\n",
       "      <td>0.768127</td>\n",
       "      <td>0.802344</td>\n",
       "      <td>0.833754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.766773</td>\n",
       "      <td>0.764143</td>\n",
       "      <td>0.812388</td>\n",
       "      <td>0.830726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "0       0.732484        0.767145      0.791728       0.828612\n",
       "1       0.767516        0.763955      0.836356       0.824302\n",
       "2       0.707006        0.779904      0.753185       0.842019\n",
       "3       0.760383        0.768127      0.802344       0.833754\n",
       "4       0.766773        0.764143      0.812388       0.830726"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculamos los estadisticos de cada experimento, tanto para train como para test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.746833</td>\n",
       "      <td>0.768655</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>0.831883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.026463</td>\n",
       "      <td>0.006549</td>\n",
       "      <td>0.030568</td>\n",
       "      <td>0.006629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.763955</td>\n",
       "      <td>0.753185</td>\n",
       "      <td>0.824302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.764143</td>\n",
       "      <td>0.791728</td>\n",
       "      <td>0.828612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.760383</td>\n",
       "      <td>0.767145</td>\n",
       "      <td>0.802344</td>\n",
       "      <td>0.830726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.766773</td>\n",
       "      <td>0.768127</td>\n",
       "      <td>0.812388</td>\n",
       "      <td>0.833754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.767516</td>\n",
       "      <td>0.779904</td>\n",
       "      <td>0.836356</td>\n",
       "      <td>0.842019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "count       5.000000        5.000000      5.000000       5.000000\n",
       "mean        0.746833        0.768655      0.799200       0.831883\n",
       "std         0.026463        0.006549      0.030568       0.006629\n",
       "min         0.707006        0.763955      0.753185       0.824302\n",
       "25%         0.732484        0.764143      0.791728       0.828612\n",
       "50%         0.760383        0.767145      0.802344       0.830726\n",
       "75%         0.766773        0.768127      0.812388       0.833754\n",
       "max         0.767516        0.779904      0.836356       0.842019"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionamos el árbol que mejor accuracy haya tenido durante el train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dtree_index = np.argmax(scores[\"test_accuracy\"]*scores[\"test_roc_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_btree = scores[\"estimator\"][best_dtree_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluamos su accuracy sobre el 20% restante del dataset que no formó parte del 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_esperado = df_test.target.values\n",
    "y_pred = best_btree.predict(df_test[feature_columns].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score validation: 0.7831632653061225\n"
     ]
    }
   ],
   "source": [
    "print(\"Score validation: \" + str(accuracy_score(y_esperado, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot_data = StringIO()\n",
    "# export_graphviz(best_btree, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True)\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volvemos a correr el 5-fold CV pero completado el dataset de train con un dado porcentaje de completitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completando columna diff_time_dayscheckout\n",
      "Moda: 0.3\n",
      "Cantidad restante de NULLs en columna diff_time_dayscheckout: (406,)\n",
      "Completando columna diff_time_daysconversion\n",
      "Moda: 1.0\n",
      "Cantidad restante de NULLs en columna diff_time_daysconversion: (1003,)\n",
      "Completando columna diff_time_daysbrand listing\n",
      "Moda: 0.07918124604762482\n",
      "Cantidad restante de NULLs en columna diff_time_daysbrand listing: (490,)\n",
      "Completando columna diff_time_dayssearch engine hit\n",
      "Moda: 0.07918124604762482\n",
      "Cantidad restante de NULLs en columna diff_time_dayssearch engine hit: (287,)\n",
      "Completando columna number_visits_model\n",
      "Moda: 0.3010299956639812\n",
      "Cantidad restante de NULLs en columna number_visits_model: (46,)\n",
      "Completando columna avg_time_events\n",
      "Moda: 0.0\n",
      "Cantidad restante de NULLs en columna avg_time_events: (5,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "c:\\python\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df_train_80 = preprocess(df_final.loc[train_indexes].copy(),\"moda\",.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1568, 28)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.loc[train_indexes].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n",
    "                        df_train_80[feature_columns].values, # X_train features\n",
    "                        df_train_80[\"target\"].values, # Y_train targets\n",
    "                        scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                        return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                        return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                        cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                        n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5_fold_resultados = pd.DataFrame(scores)\n",
    "cv_5_fold_resultados.drop([\"fit_time\",\"score_time\",\"estimator\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.655925</td>\n",
       "      <td>0.806379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.615135</td>\n",
       "      <td>0.835405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.793522</td>\n",
       "      <td>0.675556</td>\n",
       "      <td>0.850739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.848535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.740891</td>\n",
       "      <td>0.676111</td>\n",
       "      <td>0.838295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "0       0.682540        0.714286      0.655925       0.806379\n",
       "1       0.612903        0.772358      0.615135       0.835405\n",
       "2       0.639344        0.793522      0.675556       0.850739\n",
       "3       0.688525        0.789474      0.730000       0.848535\n",
       "4       0.622951        0.740891      0.676111       0.838295"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.649253</td>\n",
       "      <td>0.762106</td>\n",
       "      <td>0.670545</td>\n",
       "      <td>0.835871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.034502</td>\n",
       "      <td>0.033830</td>\n",
       "      <td>0.041458</td>\n",
       "      <td>0.017729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.615135</td>\n",
       "      <td>0.806379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.740891</td>\n",
       "      <td>0.655925</td>\n",
       "      <td>0.835405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.675556</td>\n",
       "      <td>0.838295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.676111</td>\n",
       "      <td>0.848535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.793522</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.850739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "count       5.000000        5.000000      5.000000       5.000000\n",
       "mean        0.649253        0.762106      0.670545       0.835871\n",
       "std         0.034502        0.033830      0.041458       0.017729\n",
       "min         0.612903        0.714286      0.615135       0.806379\n",
       "25%         0.622951        0.740891      0.655925       0.835405\n",
       "50%         0.639344        0.772358      0.675556       0.838295\n",
       "75%         0.682540        0.789474      0.676111       0.848535\n",
       "max         0.688525        0.793522      0.730000       0.850739"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dtree_index = np.argmax(scores[\"test_accuracy\"]*scores[\"test_roc_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_btree = scores[\"estimator\"][best_dtree_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_esperado = df_test.target.values\n",
    "y_pred = best_btree.predict(df_test[feature_columns].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score validation: 0.6045918367346939\n"
     ]
    }
   ],
   "source": [
    "print(\"Score validation: \" + str(accuracy_score(y_esperado, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot_data = StringIO()\n",
    "# export_graphviz(best_btree, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True)\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Tolerancia al ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) y b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creamos una funcion que genera un array de un largo determinado con valores 0 y un porcentage de valores random. Los valores random que se generen van a depender de la media de la columna para la cual quiere generarse ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array_noise(elements_in_column, percentage_noise, mean_column):\n",
    "    \n",
    "    number_noise = round(elements_in_column*percentage_noise)\n",
    "    \n",
    "    mu, sigma = 0, mean_column\n",
    "    \n",
    "    noise = np.random.normal(mu, sigma, [number_noise,1])\n",
    "    \n",
    "    array_noise = noise.tolist()\n",
    "    \n",
    "    array_noise_final = []\n",
    "    \n",
    "    for item in array_noise:\n",
    "        \n",
    "        array_noise_final.append(item[0])\n",
    "    \n",
    "    array_no_noise = [0]*(elements_in_column - number_noise)\n",
    "    \n",
    "    noise = array_noise_final + array_no_noise\n",
    "    \n",
    "    random.shuffle(noise)\n",
    "    \n",
    "    return noise    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creamos una funcion que, utilizando la funcion anterior, agrega un % de ruido al atributo que indiquemos de un dataset determinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_with_noise(dataframe, name_attribute, percentage_noise):\n",
    "    \n",
    "    number_observations = len(dataframe[name_attribute].values)\n",
    "    \n",
    "    mean_column_attribute = dataframe[name_attribute].mean()\n",
    "    \n",
    "    array_noise = create_array_noise(number_observations, percentage_noise, mean_column_attribute)\n",
    "    \n",
    "    dataframe[name_attribute] = dataframe[name_attribute] + array_noise\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_btree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como elegimos el mejor desicion tree?\n",
    "# https://www.kaggle.com/shotashimizu/09-decisiontree-gridsearchcv\n",
    "\n",
    "def run_model_familiy_datasets(dataframe, model, array_noise, name_attribute):\n",
    "    \n",
    "    d = {'percentage_noise': array_noise}\n",
    "    \n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    array_accuracy = []\n",
    "    \n",
    "    array_roc_auc = []\n",
    "    \n",
    "    for noise in array_noise:\n",
    "        \n",
    "        # Generamos el dataset con noise\n",
    "        \n",
    "        df_noise = generate_dataset_with_noise(dataframe, name_attribute, noise)\n",
    "        \n",
    "        # Obtenemos las predicciones del mismo\n",
    "        \n",
    "        scoring = ['accuracy','roc_auc']\n",
    "        \n",
    "        scores = cross_validate(model, # Instancia de árbol a entrenar en cada fold\n",
    "                        dataframe.drop(columns=['target']).values, # X_train features\n",
    "                        dataframe[\"target\"].values, # Y_train targets\n",
    "                        scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                        return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                        return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                        cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                        n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                       )\n",
    "                                \n",
    "        array_accuracy.append(sum(scores['test_accuracy'])/len(scores['test_accuracy']))\n",
    "                                \n",
    "        array_roc_auc.append(sum(scores['test_roc_auc'])/len(scores['test_roc_auc']))\n",
    "                    \n",
    "    df['accuracy'] = array_accuracy\n",
    "                                \n",
    "    df['roc_auc'] = array_roc_auc\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage_noise</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.746833</td>\n",
       "      <td>0.799200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.747470</td>\n",
       "      <td>0.799496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.744285</td>\n",
       "      <td>0.798659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.746833</td>\n",
       "      <td>0.800558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.746833</td>\n",
       "      <td>0.798391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.746833</td>\n",
       "      <td>0.798391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.746833</td>\n",
       "      <td>0.800542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   percentage_noise  accuracy   roc_auc\n",
       "0              0.05  0.746833  0.799200\n",
       "1              0.10  0.747470  0.799496\n",
       "2              0.15  0.744285  0.798659\n",
       "3              0.20  0.746833  0.800558\n",
       "4              0.25  0.746833  0.798391\n",
       "5              0.30  0.746833  0.798391\n",
       "6              0.35  0.746833  0.800542"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_of_noise_percentages = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35]\n",
    "\n",
    "run_model_familiy_datasets(df_train, best_btree, array_of_noise_percentages, 'diff_time_start_end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La performance del modelo no parece verse afectada por el noise. Puede ser porque no es una varaible importante para el mismo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Tenemos que usar todos los datos en df_train? Supongo que si\n",
    "\n",
    "# c) No entendi el tema de la profundidad\n",
    "\n",
    "# d) Tenemos que volver a entrar un modelo de arboles de decision y ver como performa?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DONE) Genera que solo un % de los valores del array de noise tengan otroa valores que 0\n",
    "\n",
    "# (DONE) Elegir un atributo\n",
    "\n",
    "# b) \n",
    "\n",
    "# Tenemos que generar una familia de dataset con ruido\n",
    "\n",
    "# Tenemos que correr el mejor modelo del punto dos para cada familia (es decir, le pasamos df_train y vemos que predice)\n",
    "\n",
    "# Hacer un grafico con los resultados\n",
    "\n",
    "# c)\n",
    "\n",
    "# Tenemos que entrar modelos con cada dataset aplicando cross_validation\n",
    "\n",
    "# Ver como queda la altura del arbol en cada modelo\n",
    "# Como analizamos la altura de un arbol? https://stackoverflow.com/questions/34214087/how-do-you-access-tree-depth-in-pythons-scikit-learn\n",
    "\n",
    "# d)\n",
    "\n",
    "# Ver como performa cada uno de los modelos anteriores\n",
    "\n",
    "# Graficar los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar Naive bayes sobre df_train usando la funcion de cross_validation\n",
    "\n",
    "# Predecir probabilidades target para df_test dado el modelo obtenido en el paso anterior\n",
    "\n",
    "# Probas condicionales: hacemos una funcion que nos devuelve para un atributo sus probabilidades condicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el modelo con los datos de train y predecimos sobre test. Los resultamos que vamos a obtener son las probabilidades que el modelo asigna para cada clase posible (recordemos que en este caso son solo 2: compro o no compro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00942445, 0.00892735, 0.00793743, 0.0084331 , 0.00347209]),\n",
       " 'score_time': array([0.00446486, 0.00396872, 0.00347137, 0.00347185, 0.00148821]),\n",
       " 'estimator': (GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "  GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "  GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "  GaussianNB(priors=None, var_smoothing=1e-09),\n",
       "  GaussianNB(priors=None, var_smoothing=1e-09)),\n",
       " 'test_accuracy': array([0.55414013, 0.57643312, 0.5955414 , 0.58146965, 0.55910543]),\n",
       " 'train_accuracy': array([0.60366826, 0.6092504 , 0.60287081, 0.59681275, 0.59920319]),\n",
       " 'test_roc_auc': array([0.58945959, 0.60617494, 0.65887699, 0.62671485, 0.61060755]),\n",
       " 'train_roc_auc': array([0.68958206, 0.68896139, 0.67996795, 0.68500599, 0.67967004])}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "\n",
    "scoring = ['accuracy','roc_auc']\n",
    "\n",
    "scores_nb = cross_validate(clf, # Instancia de árbol a entrenar en cada fold\n",
    "                        df_train.drop(columns=['target']).values, # X_train features\n",
    "                        df_train[\"target\"].values, # Y_train targets\n",
    "                        scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                        return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                        return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                        cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                        n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                       )\n",
    "scores_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.554140</td>\n",
       "      <td>0.603668</td>\n",
       "      <td>0.589460</td>\n",
       "      <td>0.689582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.576433</td>\n",
       "      <td>0.609250</td>\n",
       "      <td>0.606175</td>\n",
       "      <td>0.688961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.595541</td>\n",
       "      <td>0.602871</td>\n",
       "      <td>0.658877</td>\n",
       "      <td>0.679968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.581470</td>\n",
       "      <td>0.596813</td>\n",
       "      <td>0.626715</td>\n",
       "      <td>0.685006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.559105</td>\n",
       "      <td>0.599203</td>\n",
       "      <td>0.610608</td>\n",
       "      <td>0.679670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "0       0.554140        0.603668      0.589460       0.689582\n",
       "1       0.576433        0.609250      0.606175       0.688961\n",
       "2       0.595541        0.602871      0.658877       0.679968\n",
       "3       0.581470        0.596813      0.626715       0.685006\n",
       "4       0.559105        0.599203      0.610608       0.679670"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados_naive_bayes = pd.DataFrame(scores_nb)\n",
    "\n",
    "cv_5_fold_resultados_naive_bayes.drop([\"fit_time\",\"score_time\",\"estimator\"],axis=1,inplace=True)\n",
    "\n",
    "cv_5_fold_resultados_naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.573338</td>\n",
       "      <td>0.602361</td>\n",
       "      <td>0.618367</td>\n",
       "      <td>0.684637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.016881</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>0.026245</td>\n",
       "      <td>0.004737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.554140</td>\n",
       "      <td>0.596813</td>\n",
       "      <td>0.589460</td>\n",
       "      <td>0.679670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.559105</td>\n",
       "      <td>0.599203</td>\n",
       "      <td>0.606175</td>\n",
       "      <td>0.679968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.576433</td>\n",
       "      <td>0.602871</td>\n",
       "      <td>0.610608</td>\n",
       "      <td>0.685006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.581470</td>\n",
       "      <td>0.603668</td>\n",
       "      <td>0.626715</td>\n",
       "      <td>0.688961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.595541</td>\n",
       "      <td>0.609250</td>\n",
       "      <td>0.658877</td>\n",
       "      <td>0.689582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "count       5.000000        5.000000      5.000000       5.000000\n",
       "mean        0.573338        0.602361      0.618367       0.684637\n",
       "std         0.016881        0.004748      0.026245       0.004737\n",
       "min         0.554140        0.596813      0.589460       0.679670\n",
       "25%         0.559105        0.599203      0.606175       0.679968\n",
       "50%         0.576433        0.602871      0.610608       0.685006\n",
       "75%         0.581470        0.603668      0.626715       0.688961\n",
       "max         0.595541        0.609250      0.658877       0.689582"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados_naive_bayes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_naive_bayes_index = np.argmax(scores_nb[\"test_accuracy\"]*scores_nb[\"test_roc_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_naive_bayes = scores[\"estimator\"][best_naive_bayes_index]\n",
    "\n",
    "y_esperado_nb = df_test.target.values\n",
    "\n",
    "y_pred_nb = best_naive_bayes.predict(df_test[feature_columns].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy del modelo naive bayes sobre el dataset de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score validation: 0.673469387755102\n"
     ]
    }
   ],
   "source": [
    "print(\"Score validation: \" + str(accuracy_score(y_esperado_nb, y_pred_nb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_probabilities(model, dataset):\n",
    "    \n",
    "    y_pred = model.predict_proba(dataset.drop(columns=['target']))\n",
    "    \n",
    "    array_pred_class_0 = []\n",
    "    \n",
    "    array_pred_class_1 = []\n",
    "    \n",
    "    for prediction in y_pred:\n",
    "        \n",
    "        array_pred_class_0.append('{:f}'.format(prediction[0]))\n",
    "        \n",
    "        array_pred_class_1.append('{:f}'.format(prediction[1]))\n",
    "    \n",
    "    d = {'proba_class_0': array_pred_class_0, 'proba_class_1': array_pred_class_1}\n",
    "    \n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    return df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proba_class_0</th>\n",
       "      <th>proba_class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  proba_class_0 proba_class_1\n",
       "0      0.833333      0.166667\n",
       "1      0.216667      0.783333\n",
       "2      0.216667      0.783333\n",
       "3      0.833333      0.166667\n",
       "4      0.833333      0.166667"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_probabilities(best_naive_bayes, df_test).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenemos las probabilidades condicionales para un atributo del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_probabilities_attribute(dataframe, name_attribute):\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    df['target_no'] = np.where(df['target'] == 0, 1, 0)\n",
    "    \n",
    "    d = {'target': ['sum'], 'target_no': ['sum']}\n",
    "\n",
    "    df_cond = df.groupby([name_attribute]).agg(d)\n",
    "\n",
    "    df_cond.columns = ['_'.join(col) for col in df_cond.columns.values]\n",
    "\n",
    "    df_cond = df_cond.reset_index()\n",
    "\n",
    "    df_cond['target_yes'] = round(df_cond['target_sum']/df_cond['target_sum'].sum(),3)\n",
    "    \n",
    "    df_cond['target_no'] = round(df_cond['target_no_sum']/df_cond['target_no_sum'].sum(),3)\n",
    "    \n",
    "    df_cond = df_cond.drop(columns=['target_sum', 'target_no_sum'])\n",
    "    \n",
    "    return df_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Por ejemplo, dado el valor de 1 para target, la probabilidad de que 'Android' tome el valor de 0 es de 0,594 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Android  target_yes  target_no\n",
      "0      0.0       0.594      0.464\n",
      "1      1.0       0.084      0.219\n",
      "2      2.0       0.076      0.065\n",
      "3      3.0       0.039      0.056\n",
      "4      4.0       0.041      0.046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = get_conditional_probabilities_attribute(df_train, 'Android')\n",
    "\n",
    "print(df_test.head())\n",
    "\n",
    "df_test['target_yes'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analizamos si hay datos faltantes en algun columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   column  number_nulls  percentage_nulls\n",
      "0                                 Android             0               0.0\n",
      "1                                Computer             0               0.0\n",
      "2                              Smartphone             0               0.0\n",
      "3                                  Tablet             0               0.0\n",
      "4                         ad campaign hit             0               0.0\n",
      "5                           brand listing             0               0.0\n",
      "6                                checkout             0               0.0\n",
      "7                              conversion             0               0.0\n",
      "8                         generic listing             0               0.0\n",
      "9                                     iOS             0               0.0\n",
      "10                                   lead             0               0.0\n",
      "11                      search engine hit             0               0.0\n",
      "12                      searched products             0               0.0\n",
      "13                             staticpage             0               0.0\n",
      "14                                 target             0               0.0\n",
      "15                         viewed product             0               0.0\n",
      "16                           visited site             0               0.0\n",
      "17                 diff_time_dayscheckout             0               0.0\n",
      "18               diff_time_daysconversion             0               0.0\n",
      "19                    diff_time_start_end             0               0.0\n",
      "20                        avg_time_events             0               0.0\n",
      "21           diff_time_days_minnone_in_q1             0               0.0\n",
      "22           diff_time_days_minnone_in_q2             0               0.0\n",
      "23           diff_time_days_minnone_in_q3             0               0.0\n",
      "24           diff_time_days_minnone_in_q4             0               0.0\n",
      "25  diff_time_dayssearch engine hit_in_q1             0               0.0\n",
      "26  diff_time_dayssearch engine hit_in_q2             0               0.0\n",
      "27  diff_time_dayssearch engine hit_in_q3             0               0.0\n",
      "28  diff_time_dayssearch engine hit_in_q4             0               0.0\n",
      "29                COUNT(users_logs)_in_q1             0               0.0\n",
      "..                                    ...           ...               ...\n",
      "52                                  is_5s             0               0.0\n",
      "53                                  is_6S             0               0.0\n",
      "54                           is_Galaxy J5             0               0.0\n",
      "55                                   is_7             0               0.0\n",
      "56                              is_7 Plus             0               0.0\n",
      "57                           is_Galaxy S8             0               0.0\n",
      "58                           is_Galaxy S7             0               0.0\n",
      "59                      is_Galaxy S7 Edge             0               0.0\n",
      "60                     is_Galaxy J7 Prime             0               0.0\n",
      "61                     is_Moto G2 3G Dual             0               0.0\n",
      "62                      is_Galaxy S6 Edge             0               0.0\n",
      "63                      is_Galaxy S6 Flat             0               0.0\n",
      "64                                  is_5c             0               0.0\n",
      "65                             is_6S Plus             0               0.0\n",
      "66                           is_Galaxy J7             0               0.0\n",
      "67                              is_6 Plus             0               0.0\n",
      "68                        is_Moto G4 Plus             0               0.0\n",
      "69                                  is_SE             0               0.0\n",
      "70           is_Galaxy Gran Prime Duos TV             0               0.0\n",
      "71                                  is_4S             0               0.0\n",
      "72                      is_Galaxy S8 Plus             0               0.0\n",
      "73                                   is_5             0               0.0\n",
      "74                          is_Moto G3 4G             0               0.0\n",
      "75                      is_Galaxy A5 2017             0               0.0\n",
      "76                 is_Moto X Play 4G Dual             0               0.0\n",
      "77                        is_Moto G5 Plus             0               0.0\n",
      "78                      is_Galaxy A7 2017             0               0.0\n",
      "79                             is_Moto X2             0               0.0\n",
      "80                           is_Galaxy A5             0               0.0\n",
      "81                         is_other_model             0               0.0\n",
      "\n",
      "[82 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def count_missing_data(dataframe):\n",
    "\n",
    "    series_nulls = dataframe.isnull().sum()\n",
    "\n",
    "    df_nulls = pd.DataFrame({'column': series_nulls.index, 'number_nulls': series_nulls.values})\n",
    "\n",
    "    def number_rows_parent_dataframe(dataframe):\n",
    "        return len(dataframe.index)\n",
    "\n",
    "    def missing_data_porcentage(value, dataframe=dataframe):\n",
    "        return value/number_rows_parent_dataframe(dataframe)*100\n",
    "\n",
    "    df_nulls['percentage_nulls'] = df_nulls['number_nulls'].apply(missing_data_porcentage)\n",
    "\n",
    "    return df_nulls\n",
    "\n",
    "print(count_missing_data(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el dataset de \"work\", a partir del cual vamos a obtener el dataset de training y el de validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_final_reduced' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-c1d3029ef1b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_work\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_final_reduced\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mdf_final_reduced\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_work\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_final_reduced' is not defined"
     ]
    }
   ],
   "source": [
    "df_work = df_final_reduced.loc[~df_final_reduced.index.isin(df_test.index)]\n",
    "\n",
    "print(len(df_work.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work = df_work.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_work.index))\n",
    "\n",
    "df_work.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sacamos columnas que no son numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work = df_work.drop(columns=['person', 'model', 'marca', 'modelo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregamos una columna al dataset que indica a que fold pertenece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_number_fold(dataframe, number_folds):\n",
    "    \n",
    "    number_rows = len(dataframe.index)\n",
    "    \n",
    "    rows_per_fold = round(number_rows/number_folds)\n",
    "    \n",
    "    array_number_folds = []\n",
    "    \n",
    "    index = 1\n",
    "    \n",
    "    while index < (number_folds + 1):\n",
    "        \n",
    "        if (number_folds - index) == 1:\n",
    "            \n",
    "            array_numbers = [index] * (number_rows - rows_per_fold*index)\n",
    "        \n",
    "            array_number_folds = array_number_folds + array_numbers\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            array_numbers = [index] * rows_per_fold\n",
    "\n",
    "            array_number_folds = array_number_folds + array_numbers\n",
    "            \n",
    "        index = index + 1\n",
    "\n",
    "    dataframe['folds'] = array_number_folds\n",
    "    \n",
    "    return dataframe\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtramos por los folds que generamos con generate_number_fold() y vamos calculando las metricas que necesitamos tanto para training como para validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applied_cross_validation_dt(dataframe, number_folds, depth, criterion):\n",
    "    \n",
    "    df_folds = generate_number_fold(dataframe, number_folds)\n",
    "    \n",
    "    index = 1\n",
    "    \n",
    "    array_accuracy_training = []\n",
    "    \n",
    "    array_accuracy_val = []\n",
    "    \n",
    "    array_auc_training = []\n",
    "    \n",
    "    array_auc_val = []\n",
    "    \n",
    "    while index < (number_folds + 1):\n",
    "        \n",
    "        df_specific_fold = df_folds[df_folds['folds']==index].drop(columns=['folds'])\n",
    "        \n",
    "        X = df_specific_fold.drop(columns=['target'])\n",
    "\n",
    "        y = df_specific_fold['target']\n",
    "        \n",
    "        train_X, test_X, train_y, test_y = train_test_split(X, y, \n",
    "                                                    train_size=0.7,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123)\n",
    "        \n",
    "        dtree=DecisionTreeClassifier(criterion=criterion, max_depth=depth)\n",
    "        \n",
    "        dtree.fit(train_X, train_y)\n",
    "        \n",
    "        predictions_val = dtree.predict(test_X)\n",
    "        predictions_train = dtree.predict(train_X)\n",
    "        \n",
    "        proba_val = dtree.predict_proba(test_X)\n",
    "        proba_train = dtree.predict_proba(train_X)\n",
    "        \n",
    "        acc_val = accuracy_score(test_y, predictions_val)\n",
    "        acc_train = accuracy_score(train_y, predictions_train)\n",
    "        \n",
    "        auc_val = roc_auc_score(test_y, proba_val[:,1])\n",
    "        auc_train = roc_auc_score(train_y, proba_train[:,1])\n",
    "        \n",
    "        array_accuracy_val.append(acc_val)\n",
    "        \n",
    "        array_accuracy_training.append(acc_train)\n",
    "        \n",
    "        array_auc_training.append(auc_val)\n",
    "    \n",
    "        array_auc_val.append(auc_train)\n",
    "        \n",
    "        index = index + 1\n",
    "    \n",
    "    return array_accuracy_training, array_accuracy_val, array_auc_training, array_auc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applied_cross_validation_dt(df_work, 5, 3, 'gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table(acc_train, acc_val, auc_train, auc_val, criterion, depth, number_folds):\n",
    "    \n",
    "    d = {'acc_train': acc_train, 'acc_val': acc_val, 'auc_train': auc_train, 'auc_val': auc_val}\n",
    "    \n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    df['criterion'] = [criterion] * number_folds\n",
    "    \n",
    "    df['max_depth'] = [depth] * number_folds\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_gini = applied_cross_validation_dt(df_work, 5, 3, 'gini')[0]\n",
    "\n",
    "acc_val_gini = applied_cross_validation_dt(df_work, 5, 3, 'gini')[1]\n",
    "\n",
    "auc_train_gini = applied_cross_validation_dt(df_work, 5, 3, 'gini')[2]\n",
    "\n",
    "auc_val_gini = applied_cross_validation_dt(df_work, 5, 3, 'gini')[3]\n",
    "\n",
    "df_gini_3 = generate_table(acc_train_gini, acc_val_gini, auc_train_gini, auc_val_gini, 'Gini', 3, 5)\n",
    "\n",
    "df_gini_3 = df_gini_3.append(df_gini_3.mean(numeric_only=True), ignore_index=True)\n",
    "\n",
    "df_gini_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_total_row(s):\n",
    "    if s.B > 1.0:\n",
    "        return ['background-color: yellow']*5\n",
    " \n",
    "df.style.apply(highlight_greaterthan_1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_gini = applied_cross_validation_dt(df_work, 5, 6, 'gini')[0]\n",
    "\n",
    "acc_val_gini = applied_cross_validation_dt(df_work, 5, 6, 'gini')[1]\n",
    "\n",
    "auc_train_gini = applied_cross_validation_dt(df_work, 5, 6, 'gini')[2]\n",
    "\n",
    "auc_val_gini = applied_cross_validation_dt(df_work, 5, 6, 'gini')[3]\n",
    "\n",
    "generate_table(acc_train_gini, acc_val_gini, auc_train_gini, auc_val_gini, 'Gini', 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
