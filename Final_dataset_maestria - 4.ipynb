{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos todas las bibliotecas necesarias para ejecutar todos los métodos del jupyter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es buena costumbre cargarlos al principio, asi no se estar cargando en memoria repetidas veces cada paquete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(df, column):\n",
    "    q1 = df_final[column].quantile(.25)\n",
    "    q2 = df_final[column].quantile(.5)\n",
    "    q3 = df_final[column].quantile(.75)\n",
    "    df[column+\"_in_q1\"] = 0\n",
    "    df[column+\"_in_q2\"] = 0\n",
    "    df[column+\"_in_q3\"] = 0\n",
    "    df[column+\"_in_q4\"] = 0\n",
    "    df[column+\"_in_q1\"] = list(map(lambda x: 1 if x <=q1 else 0, df[column]))\n",
    "    df[column+\"_in_q2\"] = list(map(lambda x: 1 if (q1 < x and x <=q2) else 0, df[column]))\n",
    "    df[column+\"_in_q3\"] = list(map(lambda x: 1 if (q2 < x and x <=q3) else 0, df[column]))\n",
    "    df[column+\"_in_q4\"] = list(map(lambda x: 1 if x>q3 else 0, df[column]))\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_object(df,column,object_name):\n",
    "    df[\"is_\"+str(object_name)] = list(map(lambda x: 1 if x == object_name else 0, df[column]))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_replace_nans(df,col,rate_to_complete):\n",
    "    \n",
    "    index_list = df[col][df[col].isnull()].sample(frac=rate_to_complete,random_state=1).index.tolist()\n",
    "    \n",
    "    if len(index_list)>0:\n",
    "        moda = df[(~df[col].isnull())][col].mode()[0]\n",
    "        df.loc[index_list,col] = moda\n",
    "        print(\"Completando columna \" + str(col))\n",
    "        print(\"Moda: \" + str(moda))\n",
    "        print(\"Cantidad restante de NULLs en columna \" + str(col) + \": \"+str(df[col][df[col].isnull()].shape))\n",
    "        \n",
    "    return df\n",
    "    \n",
    "def complete_nans(df,null_cols,rate_to_complete,method):\n",
    "    if method==\"moda_por_clase\":\n",
    "        for col in null_cols:\n",
    "            print(\"Completando clase 0\")\n",
    "            df0 = find_and_replace_nans(df[df.target==0.0],col,rate_to_complete)\n",
    "            print(\"Completando clase 1\")\n",
    "            df1 = find_and_replace_nans(df[df.target==1.0],col,rate_to_complete)\n",
    "            df = pd.concat([df0,df1])\n",
    "    else:\n",
    "        for col in null_cols:\n",
    "            df = find_and_replace_nans(df,col,rate_to_complete)\n",
    "    \n",
    "    return df.dropna()\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, metodo, rate_to_complete):\n",
    "    \n",
    "    # Extraemos las filas de la columna 'model' que tienen valores NaNs y luego los completamos con un valor por defecto\n",
    "    \n",
    "    #model_null_indexes = df[df.model.isnull()].index.tolist()\n",
    "    #df.loc[model_null_indexes,\"model\"] = \"other other\"\n",
    "    \n",
    "    df['model'] = df['model'].fillna('other other')\n",
    "    \n",
    "    # Obtenemos las columnas que tienen al menos un NaN\n",
    "    cols_in_null = df.columns[df.isnull().sum(axis=0)>0].tolist()\n",
    "    \n",
    "    # Los completamos con un rate_to_complete y utilizando un determinado metodo\n",
    "    df_comp = complete_nans(df,cols_in_null,rate_to_complete,metodo)\n",
    "    \n",
    "    # Discretizamos las columnas que ya sabemos que tienen una distribucion dada (Chi, Normal)\n",
    "    # (Se usan cuantiles para ver en qué segmento caen)\n",
    "    for column in [\"diff_time_days_minnone\",\"diff_time_dayssearch engine hit\",\"COUNT(users_logs)\",\"number_visits_model\",\"diff_time_daysbrand listing\",\"diff_time_daysnone\"]:\n",
    "        df_comp = discretize(df_comp,column)\n",
    "    \n",
    "    # De la columna 'model' extraemos marca y modelo del celular visitado o  comprado\n",
    "    df_comp[\"marca\"] = df_comp.model.str.split(\" \",1,expand=True)[0]\n",
    "    df_comp[\"modelo\"] = df_comp.model.str.split(\" \",1,expand=True)[1]\n",
    "    \n",
    "    # Determinamos de que marca se trata\n",
    "    df_comp[\"is_iphone\"] = list(map(lambda x: 1 if x==\"iPhone\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_samsung\"] = list(map(lambda x: 1 if x==\"Samsung\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_motorola\"] = list(map(lambda x: 1 if x==\"Motorola\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_lg\"] = list(map(lambda x: 1 if x==\"LG\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_sony\"] = list(map(lambda x: 1 if x==\"Sony\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_other\"] = list(map(lambda x: 1 if (x!=\"Sony\" and x!=\"LG\" and x!=\"Motorola\" and x!=\"Samsung\" and x!=\"iPhone\") else 0, df_comp[\"marca\"]))\n",
    "    \n",
    "    # determinamos de que modelo se trata\n",
    "    pop_models = ['6','5s','6S','Galaxy J5','7','7 Plus','Galaxy S8','Galaxy S7','Galaxy S7 Edge','Galaxy J7 Prime','Moto G2 3G Dual','Galaxy S6 Edge','Galaxy S6 Flat','5c','6S Plus','Galaxy J7','6 Plus','Moto G4 Plus','SE','Galaxy Gran Prime Duos TV','4S','Galaxy S8 Plus','5','Moto G3 4G','Galaxy A5 2017','Moto X Play 4G Dual','Moto G5 Plus','Galaxy A7 2017','Moto X2','Galaxy A5']\n",
    "    for model in pop_models:\n",
    "        df_comp = is_object(df_comp,\"modelo\",model)\n",
    "\n",
    "    df_comp[\"is_other_model\"] = list(map(lambda x: 1 if x not in pop_models else 0, df_comp[\"modelo\"]))\n",
    "    \n",
    "    # Eliminamos las columnas con strings\n",
    "    df_comp.drop([\"model\",\"marca\",\"modelo\"],axis=1,inplace=True)\n",
    "    \n",
    "    return df_comp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos los datasets para construir el dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first = pd.read_csv('../../data_aa/first_part_dataset.csv')\n",
    "\n",
    "df_second = pd.read_csv('../../data_aa/second_part_dataset.csv')\n",
    "\n",
    "df_thrid = pd.read_csv('../../data_aa/thrid_part_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first = df_first.rename(columns={'person_id': 'person'})\n",
    "\n",
    "df_first = df_first.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'person', 'diff_time_days_minnone',\n",
       "       'diff_time_daysnone', 'diff_time_dayscheckout',\n",
       "       'diff_time_daysconversion', 'diff_time_daysbrand listing',\n",
       "       'diff_time_dayssearch engine hit', 'diff_time_start_end', 'model',\n",
       "       'number_visits_model'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_second.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second = df_second.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'person', 'COUNT(users_logs)', 'avg_time_events'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_thrid.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thrid = df_thrid.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_first, df_second, on='person', how='left')\n",
    "\n",
    "df_final = pd.merge(df_final, df_thrid, on='person', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19414,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.target.isin([1.0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significado de las columnas:\n",
    "\n",
    " - Las columnas 'Android', 'BlackBerry', 'Chrome', 'FreeBSD', 'Linux', 'Mac', 'Tizen', 'Ubuntu', 'Unknown', 'Windows','iOS' indican la cantidad de veces que cada usuario ingreso a la pagina de Trocafone utilizando uno de esos esos sistemas operativos. Por ejemplo, si el valor de 'Android' para un usuario es 2 siginfica que ese usuario entro a la pagina dos veces desde un dispositivo con un sistema operativo Android\n",
    " \n",
    " - Las columnas 'Computer', 'Smartphone', 'Tablet' indican la cantidad de veces que cada usuario ingreso a la pagina de Trocafone utilizando uno de esos dispositivos\n",
    " \n",
    " - Las columnas 'ad campaign hit', 'brand listing', 'checkout', 'conversion', 'generic listing', 'lead','search engine hit', 'searched products', 'staticpage','viewed product', 'visited site' indican la cantidad de veces que cada usuario realizo cada uno de esos eventos\n",
    " \n",
    " - La columna 'diff_time_days_minnone' indica la cantidad de tiempo que paso desde el primer evento que realizo el usuario hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_daysnone' indica la cantidad de tiempo que paso desde el ultimo evento que el usuario realizo en la pagina hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_dayscheckout' indica la cantidad de tiempo que paso desde que el usuario realizo el ultimo checkout hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_daysconversion' indica la cantidad de tiempo que paso desde que el usuario realizo la ultima conversion hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_daysbrand listing' indica la cantidad de tiempo que paso desde que el usuario realizo la ultima brand listing hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_dayssearch engine hit' indica la cantidad de tiempo que paso desde que el usuario realizo el ultimo engine hit hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_start_end' indica la cantidad de tiempo que paso entre el primer evento que realizo el usuario en la pagina y el ultimo\n",
    " \n",
    " - La columna 'model' indica el modelo de celuar que el usuario mas veces visito\n",
    " \n",
    " - La columna 'number_visits_model' indica cuantas veces el usuario visito el celular que mas visito\n",
    " \n",
    " - La columna 'COUNT(users_logs)' indica la cantidad de eventos que el usuario realizo en la pagina\n",
    " \n",
    " - La columna 'avg_time_events' indica el tiempo promedio que paso entre los eventos realizados por cada usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminamos las columnas con muy pocas variables o alta corrleación (Ej. Windows ~ Computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop([\"BlackBerry\",\"Chrome\",\"FreeBSD\",\"Linux\",\"Mac\",\"Other\",\"Tizen\",\"Ubuntu\",\"Unknown\",\"Windows\",\"person\"],\n",
    "              axis=1,\n",
    "              inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformamos algunas columnas para que su distribución se ajuste a una Chi cuadrado o Normal. Luego discretizamos por cuantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.loc[:,\"diff_time_daysnone\"] = np.log10(df_final[\"diff_time_daysnone\"]+1.)\n",
    "df_final.loc[:,\"diff_time_days_minnone\"] = np.log10(df_final.diff_time_days_minnone+1.)\n",
    "df_final.loc[:,\"diff_time_dayssearch engine hit\"] = np.log10(df_final[\"diff_time_dayssearch engine hit\"]+1.0)\n",
    "df_final.loc[:,\"COUNT(users_logs)\"] = np.log10(df_final[\"COUNT(users_logs)\"]+1.)\n",
    "df_final.loc[:,\"number_visits_model\"] = np.log10(1.+df_final.number_visits_model)\n",
    "df_final.loc[:,\"diff_time_daysbrand listing\"] = np.log10(01.+df_final[\"diff_time_daysbrand listing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         NaN\n",
       "1                    iPhone 7\n",
       "2                   iPhone 6S\n",
       "3    Motorola Moto G2 3G Dual\n",
       "4      Samsung Galaxy S8 Plus\n",
       "Name: model, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['model'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En el siguiente metodo  se realiza todo el preprocess de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completando columna diff_time_dayscheckout\n",
      "Moda: 2.2\n",
      "Cantidad restante de NULLs en columna diff_time_dayscheckout: (0,)\n",
      "Completando columna diff_time_daysconversion\n",
      "Moda: 1.2\n",
      "Cantidad restante de NULLs en columna diff_time_daysconversion: (0,)\n",
      "Completando columna diff_time_daysbrand listing\n",
      "Moda: 0.11394335230683679\n",
      "Cantidad restante de NULLs en columna diff_time_daysbrand listing: (0,)\n",
      "Completando columna diff_time_dayssearch engine hit\n",
      "Moda: 0.11394335230683679\n",
      "Cantidad restante de NULLs en columna diff_time_dayssearch engine hit: (0,)\n",
      "Completando columna number_visits_model\n",
      "Moda: 0.3010299956639812\n",
      "Cantidad restante de NULLs en columna number_visits_model: (0,)\n",
      "Completando columna avg_time_events\n",
      "Moda: 0.0\n",
      "Cantidad restante de NULLs en columna avg_time_events: (0,)\n"
     ]
    }
   ],
   "source": [
    "df_completo = preprocess(df_final.copy(),\"moda\",1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a seleccionar solo 980 casos que no realizaron una compra para balancear el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    }
   ],
   "source": [
    "df_completo_reduced_1 = df_completo[df_completo['target']==1]\n",
    "print(len(df_completo_reduced_1.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    }
   ],
   "source": [
    "df_completo_reduced_0 = df_completo[df_completo['target']==0].sample(n=980, random_state=1)\n",
    "print(len(df_completo_reduced_0.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_completo_reduced_1, df_completo_reduced_0])\n",
    "del df_completo_reduced_0, df_completo_reduced_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nos quedamos con aquellas columnas que no sean target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = list(set(df_train.columns.tolist())-set([\"target\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenemos el dataset de test y lo eliminamos del dataset de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_train.sample(frac=0.2, random_state=2)\n",
    "df_train.drop(df_test.index,axis=0,inplace=True)\n",
    "train_indexes = df_train.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificamos el balance de categorías en cada datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = df_train.shape[0]*1.\n",
    "N_test = df_test.shape[0]*1.\n",
    "\n",
    "N_train_1 = df_train.target.sum()\n",
    "N_train_0 = df_train.shape[0]*1.-N_train_1\n",
    "\n",
    "N_test_1 = df_test.target.sum()\n",
    "N_test_0 = df_test.shape[0]*1.-N_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de casos positivos en el train: %50.255102040816325\n",
      "Porcentaje de casos positivos en el test: %48.97959183673469\n"
     ]
    }
   ],
   "source": [
    "print(\"Porcentaje de casos positivos en el train: %\" + str(100.0*N_train_1/N_train))\n",
    "print(\"Porcentaje de casos positivos en el test: %\" + str(100.0*N_test_1/N_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando únicamente el dataset de train ejecutamos 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy','roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(max_depth=3, criterion='gini') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n",
    "                        df_train[feature_columns].values, # X_train features\n",
    "                        df_train[\"target\"].values, # Y_train targets\n",
    "                        scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                        return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                        return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                        cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                        n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los resultados en un DF de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5_fold_resultados = pd.DataFrame(scores)\n",
    "cv_5_fold_resultados.drop([\"fit_time\",\"score_time\",\"estimator\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.767145</td>\n",
       "      <td>0.791728</td>\n",
       "      <td>0.828612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.767516</td>\n",
       "      <td>0.763955</td>\n",
       "      <td>0.836356</td>\n",
       "      <td>0.824302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.779904</td>\n",
       "      <td>0.753185</td>\n",
       "      <td>0.842019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.760383</td>\n",
       "      <td>0.768127</td>\n",
       "      <td>0.802344</td>\n",
       "      <td>0.833754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.769968</td>\n",
       "      <td>0.764143</td>\n",
       "      <td>0.814368</td>\n",
       "      <td>0.830726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "0       0.732484        0.767145      0.791728       0.828612\n",
       "1       0.767516        0.763955      0.836356       0.824302\n",
       "2       0.707006        0.779904      0.753185       0.842019\n",
       "3       0.760383        0.768127      0.802344       0.833754\n",
       "4       0.769968        0.764143      0.814368       0.830726"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculamos los estadisticos de cada experimento, tanto para train como para test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.747472</td>\n",
       "      <td>0.768655</td>\n",
       "      <td>0.799596</td>\n",
       "      <td>0.831883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.027095</td>\n",
       "      <td>0.006549</td>\n",
       "      <td>0.030793</td>\n",
       "      <td>0.006629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.763955</td>\n",
       "      <td>0.753185</td>\n",
       "      <td>0.824302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.764143</td>\n",
       "      <td>0.791728</td>\n",
       "      <td>0.828612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.760383</td>\n",
       "      <td>0.767145</td>\n",
       "      <td>0.802344</td>\n",
       "      <td>0.830726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.767516</td>\n",
       "      <td>0.768127</td>\n",
       "      <td>0.814368</td>\n",
       "      <td>0.833754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.769968</td>\n",
       "      <td>0.779904</td>\n",
       "      <td>0.836356</td>\n",
       "      <td>0.842019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "count       5.000000        5.000000      5.000000       5.000000\n",
       "mean        0.747472        0.768655      0.799596       0.831883\n",
       "std         0.027095        0.006549      0.030793       0.006629\n",
       "min         0.707006        0.763955      0.753185       0.824302\n",
       "25%         0.732484        0.764143      0.791728       0.828612\n",
       "50%         0.760383        0.767145      0.802344       0.830726\n",
       "75%         0.767516        0.768127      0.814368       0.833754\n",
       "max         0.769968        0.779904      0.836356       0.842019"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionamos el árbol que mejor accuracy haya tenido durante el train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dtree_index = np.argmax(scores[\"test_accuracy\"]*scores[\"test_roc_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_btree = scores[\"estimator\"][best_dtree_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluamos su accuracy sobre el 20% restante del dataset que no formó parte del 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_esperado = df_test.target.values\n",
    "y_pred = best_btree.predict(df_test[feature_columns].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score validation: 0.7831632653061225\n"
     ]
    }
   ],
   "source": [
    "print(\"Score validation: \" + str(accuracy_score(y_esperado, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot_data = StringIO()\n",
    "# export_graphviz(best_btree, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True)\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volvemos a correr el 5-fold CV pero completado el dataset de train con un dado porcentaje de completitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completando columna diff_time_dayscheckout\n",
      "Moda: 0.3\n",
      "Cantidad restante de NULLs en columna diff_time_dayscheckout: (406,)\n",
      "Completando columna diff_time_daysconversion\n",
      "Moda: 1.0\n",
      "Cantidad restante de NULLs en columna diff_time_daysconversion: (1003,)\n",
      "Completando columna diff_time_daysbrand listing\n",
      "Moda: 0.07918124604762482\n",
      "Cantidad restante de NULLs en columna diff_time_daysbrand listing: (490,)\n",
      "Completando columna diff_time_dayssearch engine hit\n",
      "Moda: 0.07918124604762482\n",
      "Cantidad restante de NULLs en columna diff_time_dayssearch engine hit: (287,)\n",
      "Completando columna number_visits_model\n",
      "Moda: 0.3010299956639812\n",
      "Cantidad restante de NULLs en columna number_visits_model: (46,)\n",
      "Completando columna avg_time_events\n",
      "Moda: 0.0\n",
      "Cantidad restante de NULLs en columna avg_time_events: (5,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\bangho\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df_train_80 = preprocess(df_final.loc[train_indexes].copy(),\"moda\",.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1568, 28)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.loc[train_indexes].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n",
    "                        df_train_80[feature_columns].values, # X_train features\n",
    "                        df_train_80[\"target\"].values, # Y_train targets\n",
    "                        scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                        return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                        return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                        cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                        n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5_fold_resultados = pd.DataFrame(scores)\n",
    "cv_5_fold_resultados.drop([\"fit_time\",\"score_time\",\"estimator\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.655925</td>\n",
       "      <td>0.806379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.615135</td>\n",
       "      <td>0.835405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.793522</td>\n",
       "      <td>0.675556</td>\n",
       "      <td>0.850739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.848535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.740891</td>\n",
       "      <td>0.676111</td>\n",
       "      <td>0.838295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "0       0.682540        0.714286      0.655925       0.806379\n",
       "1       0.612903        0.772358      0.615135       0.835405\n",
       "2       0.639344        0.793522      0.675556       0.850739\n",
       "3       0.688525        0.789474      0.730000       0.848535\n",
       "4       0.622951        0.740891      0.676111       0.838295"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.649253</td>\n",
       "      <td>0.762106</td>\n",
       "      <td>0.670545</td>\n",
       "      <td>0.835871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.034502</td>\n",
       "      <td>0.033830</td>\n",
       "      <td>0.041458</td>\n",
       "      <td>0.017729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.615135</td>\n",
       "      <td>0.806379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.740891</td>\n",
       "      <td>0.655925</td>\n",
       "      <td>0.835405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.675556</td>\n",
       "      <td>0.838295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.676111</td>\n",
       "      <td>0.848535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.793522</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.850739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "count       5.000000        5.000000      5.000000       5.000000\n",
       "mean        0.649253        0.762106      0.670545       0.835871\n",
       "std         0.034502        0.033830      0.041458       0.017729\n",
       "min         0.612903        0.714286      0.615135       0.806379\n",
       "25%         0.622951        0.740891      0.655925       0.835405\n",
       "50%         0.639344        0.772358      0.675556       0.838295\n",
       "75%         0.682540        0.789474      0.676111       0.848535\n",
       "max         0.688525        0.793522      0.730000       0.850739"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dtree_index = np.argmax(scores[\"test_accuracy\"]*scores[\"test_roc_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_btree = scores[\"estimator\"][best_dtree_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_esperado = df_test.target.values\n",
    "y_pred = best_btree.predict(df_test[feature_columns].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score validation: 0.6045918367346939\n"
     ]
    }
   ],
   "source": [
    "print(\"Score validation: \" + str(accuracy_score(y_esperado, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot_data = StringIO()\n",
    "# export_graphviz(best_btree, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True)\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Tolerancia al ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creamos una funcion que genera un array con valores 0 y numeros random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array_noise(elements_in_column, percentage_noise):\n",
    "    \n",
    "    number_noise = round(elements_in_column*percentage_noise)\n",
    "    \n",
    "    mu, sigma = 0, 0.1\n",
    "    \n",
    "    noise = np.random.normal(mu, sigma, [number_noise,1])\n",
    "    \n",
    "    array_noise = noise.tolist()\n",
    "    \n",
    "    array_noise_final = []\n",
    "    \n",
    "    for item in array_noise:\n",
    "        \n",
    "        array_noise_final.append(item[0])\n",
    "    \n",
    "    array_no_noise = [0]*(elements_in_column - number_noise)\n",
    "    \n",
    "    noise = array_noise_final + array_no_noise\n",
    "    \n",
    "    random.shuffle(noise)\n",
    "    \n",
    "    return noise    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creamos una funcion que, utilizando la funcion anterior, agrega un % de ruido al atributo que indiquemos de un dataset determinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_with_noise(dataframe, name_attribute, percentage_noise):\n",
    "    \n",
    "    number_observations = len(dataframe[name_attribute].values)\n",
    "    \n",
    "    array_noise = create_array_noise(number_observations, percentage_noise)\n",
    "    \n",
    "    dataframe[name_attribute] = dataframe[name_attribute] + array_noise\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_of_noise_percentages = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Tenemos que usar todos los datos en df_train? Supongo que si\n",
    "\n",
    "# c) No entendi el tema de la profundidad\n",
    "\n",
    "# d) Tenemos que volver a entrar un modelo de arboles de decision y ver como performa?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13     4.000000\n",
       "17     3.047724\n",
       "78     0.993775\n",
       "127    1.000000\n",
       "254    7.000000\n",
       "Name: visited site, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_dataset_with_noise(df_train, 'visited site', 0.1)['visited site'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13     4.0\n",
       "17     3.0\n",
       "78     1.0\n",
       "127    1.0\n",
       "254    7.0\n",
       "Name: visited site, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['visited site'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A    B\n",
      "0  2.980688  2.0\n",
      "1  4.877631  4.0\n"
     ]
    }
   ],
   "source": [
    "clean_signal['A'] = clean_signal['A'] + create_array_noise(2)\n",
    "print(clean_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DONE) Genera que solo un % de los valores del array de noise tengan otroa valores que 0\n",
    "\n",
    "# (DONE) Elegir un atributo\n",
    "\n",
    "# Tenemos que generar una familia de dataset con ruido\n",
    "\n",
    "# Tenemos que correr el mejor modelo del punto dos para cada familia y obtener los resultados sobre el set de test\n",
    "\n",
    "# Hacer un grafico con los resultados\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el dataset de \"work\", a partir del cual vamos a obtener el dataset de training y el de validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_final_reduced' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-c1d3029ef1b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_work\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_final_reduced\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mdf_final_reduced\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_work\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_final_reduced' is not defined"
     ]
    }
   ],
   "source": [
    "df_work = df_final_reduced.loc[~df_final_reduced.index.isin(df_test.index)]\n",
    "\n",
    "print(len(df_work.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work = df_work.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_work.index))\n",
    "\n",
    "df_work.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sacamos columnas que no son numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work = df_work.drop(columns=['person', 'model', 'marca', 'modelo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregamos una columna al dataset que indica a que fold pertenece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_number_fold(dataframe, number_folds):\n",
    "    \n",
    "    number_rows = len(dataframe.index)\n",
    "    \n",
    "    rows_per_fold = round(number_rows/number_folds)\n",
    "    \n",
    "    array_number_folds = []\n",
    "    \n",
    "    index = 1\n",
    "    \n",
    "    while index < (number_folds + 1):\n",
    "        \n",
    "        if (number_folds - index) == 1:\n",
    "            \n",
    "            array_numbers = [index] * (number_rows - rows_per_fold*index)\n",
    "        \n",
    "            array_number_folds = array_number_folds + array_numbers\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            array_numbers = [index] * rows_per_fold\n",
    "\n",
    "            array_number_folds = array_number_folds + array_numbers\n",
    "            \n",
    "        index = index + 1\n",
    "\n",
    "    dataframe['folds'] = array_number_folds\n",
    "    \n",
    "    return dataframe\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtramos por los folds que generamos con generate_number_fold() y vamos calculando las metricas que necesitamos tanto para training como para validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applied_cross_validation_dt(dataframe, number_folds, depth, criterion):\n",
    "    \n",
    "    df_folds = generate_number_fold(dataframe, number_folds)\n",
    "    \n",
    "    index = 1\n",
    "    \n",
    "    array_accuracy_training = []\n",
    "    \n",
    "    array_accuracy_val = []\n",
    "    \n",
    "    array_auc_training = []\n",
    "    \n",
    "    array_auc_val = []\n",
    "    \n",
    "    while index < (number_folds + 1):\n",
    "        \n",
    "        df_specific_fold = df_folds[df_folds['folds']==index].drop(columns=['folds'])\n",
    "        \n",
    "        X = df_specific_fold.drop(columns=['target'])\n",
    "\n",
    "        y = df_specific_fold['target']\n",
    "        \n",
    "        train_X, test_X, train_y, test_y = train_test_split(X, y, \n",
    "                                                    train_size=0.7,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123)\n",
    "        \n",
    "        dtree=DecisionTreeClassifier(criterion=criterion, max_depth=depth)\n",
    "        \n",
    "        dtree.fit(train_X, train_y)\n",
    "        \n",
    "        predictions_val = dtree.predict(test_X)\n",
    "        predictions_train = dtree.predict(train_X)\n",
    "        \n",
    "        proba_val = dtree.predict_proba(test_X)\n",
    "        proba_train = dtree.predict_proba(train_X)\n",
    "        \n",
    "        acc_val = accuracy_score(test_y, predictions_val)\n",
    "        acc_train = accuracy_score(train_y, predictions_train)\n",
    "        \n",
    "        auc_val = roc_auc_score(test_y, proba_val[:,1])\n",
    "        auc_train = roc_auc_score(train_y, proba_train[:,1])\n",
    "        \n",
    "        array_accuracy_val.append(acc_val)\n",
    "        \n",
    "        array_accuracy_training.append(acc_train)\n",
    "        \n",
    "        array_auc_training.append(auc_val)\n",
    "    \n",
    "        array_auc_val.append(auc_train)\n",
    "        \n",
    "        index = index + 1\n",
    "    \n",
    "    return array_accuracy_training, array_accuracy_val, array_auc_training, array_auc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applied_cross_validation_dt(df_work, 5, 3, 'gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table(acc_train, acc_val, auc_train, auc_val, criterion, depth, number_folds):\n",
    "    \n",
    "    d = {'acc_train': acc_train, 'acc_val': acc_val, 'auc_train': auc_train, 'auc_val': auc_val}\n",
    "    \n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    df['criterion'] = [criterion] * number_folds\n",
    "    \n",
    "    df['max_depth'] = [depth] * number_folds\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_gini = applied_cross_validation_dt(df_work, 5, 3, 'gini')[0]\n",
    "\n",
    "acc_val_gini = applied_cross_validation_dt(df_work, 5, 3, 'gini')[1]\n",
    "\n",
    "auc_train_gini = applied_cross_validation_dt(df_work, 5, 3, 'gini')[2]\n",
    "\n",
    "auc_val_gini = applied_cross_validation_dt(df_work, 5, 3, 'gini')[3]\n",
    "\n",
    "df_gini_3 = generate_table(acc_train_gini, acc_val_gini, auc_train_gini, auc_val_gini, 'Gini', 3, 5)\n",
    "\n",
    "df_gini_3 = df_gini_3.append(df_gini_3.mean(numeric_only=True), ignore_index=True)\n",
    "\n",
    "df_gini_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_total_row(s):\n",
    "    if s.B > 1.0:\n",
    "        return ['background-color: yellow']*5\n",
    " \n",
    "df.style.apply(highlight_greaterthan_1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_gini = applied_cross_validation_dt(df_work, 5, 6, 'gini')[0]\n",
    "\n",
    "acc_val_gini = applied_cross_validation_dt(df_work, 5, 6, 'gini')[1]\n",
    "\n",
    "auc_train_gini = applied_cross_validation_dt(df_work, 5, 6, 'gini')[2]\n",
    "\n",
    "auc_val_gini = applied_cross_validation_dt(df_work, 5, 6, 'gini')[3]\n",
    "\n",
    "generate_table(acc_train_gini, acc_val_gini, auc_train_gini, auc_val_gini, 'Gini', 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
