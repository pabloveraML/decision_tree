{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos todas las bibliotecas necesarias para ejecutar todos los métodos del jupyter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es buena costumbre cargarlos al principio, asi no se estar cargando en memoria repetidas veces cada paquete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "#import pydotplus\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = [30, 10]\n",
    "plt.rcParams['font.size'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(df, column):\n",
    "    q1 = df_final[column].quantile(.25)\n",
    "    q2 = df_final[column].quantile(.5)\n",
    "    q3 = df_final[column].quantile(.75)\n",
    "    df[column+\"_in_q1\"] = 0\n",
    "    df[column+\"_in_q2\"] = 0\n",
    "    df[column+\"_in_q3\"] = 0\n",
    "    df[column+\"_in_q4\"] = 0\n",
    "    df[column+\"_in_q1\"] = list(map(lambda x: 1 if x <=q1 else 0, df[column]))\n",
    "    df[column+\"_in_q2\"] = list(map(lambda x: 1 if (q1 < x and x <=q2) else 0, df[column]))\n",
    "    df[column+\"_in_q3\"] = list(map(lambda x: 1 if (q2 < x and x <=q3) else 0, df[column]))\n",
    "    df[column+\"_in_q4\"] = list(map(lambda x: 1 if x>q3 else 0, df[column]))\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_object(df,column,object_name):\n",
    "    df[\"is_\"+str(object_name)] = list(map(lambda x: 1 if x == object_name else 0, df[column]))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_replace_nans(df,col,rate_to_complete):\n",
    "    \n",
    "    index_list = df[col][df[col].isnull()].sample(frac=rate_to_complete,random_state=1).index.tolist()\n",
    "    \n",
    "    if len(index_list)>0:\n",
    "        moda = df[(~df[col].isnull())][col].mode()[0]\n",
    "        df.loc[index_list,col] = moda\n",
    "    return df\n",
    "    \n",
    "def complete_nans(df,null_cols,rate_to_complete,method):\n",
    "    if method==\"moda_por_clase\":\n",
    "        for col in null_cols:\n",
    "            df0 = find_and_replace_nans(df[df.target==0.0],col,rate_to_complete)\n",
    "            df1 = find_and_replace_nans(df[df.target==1.0],col,rate_to_complete)\n",
    "            df = pd.concat([df0,df1])\n",
    "    else:\n",
    "        for col in null_cols:\n",
    "            df = find_and_replace_nans(df,col,rate_to_complete)\n",
    "    \n",
    "    return df.dropna()\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, metodo, rate_to_complete):\n",
    "    \n",
    "    # Extraemos las filas de la columna 'model' que tienen valores NaNs y luego los completamos con un valor por defecto\n",
    "    \n",
    "    #model_null_indexes = df[df.model.isnull()].index.tolist()\n",
    "    #df.loc[model_null_indexes,\"model\"] = \"other other\"\n",
    "    \n",
    "    df['model'] = df['model'].fillna('other other')\n",
    "    \n",
    "    # Obtenemos las columnas que tienen al menos un NaN\n",
    "    cols_in_null = df.columns[df.isnull().sum(axis=0)>0].tolist()\n",
    "    \n",
    "    # Los completamos con un rate_to_complete y utilizando un determinado metodo\n",
    "    df_comp = complete_nans(df,cols_in_null,rate_to_complete,metodo)\n",
    "    \n",
    "    # Discretizamos las columnas que ya sabemos que tienen una distribucion dada (Chi, Normal)\n",
    "    # (Se usan cuantiles para ver en qué segmento caen)\n",
    "    for column in [\"diff_time_days_minnone\",\"diff_time_dayssearch engine hit\",\"COUNT(users_logs)\",\"number_visits_model\",\"diff_time_daysbrand listing\",\"diff_time_daysnone\"]:\n",
    "        df_comp = discretize(df_comp,column)\n",
    "    \n",
    "    # De la columna 'model' extraemos marca y modelo del celular visitado o  comprado\n",
    "    df_comp[\"marca\"] = df_comp.model.str.split(\" \",1,expand=True)[0]\n",
    "    df_comp[\"modelo\"] = df_comp.model.str.split(\" \",1,expand=True)[1]\n",
    "    \n",
    "    # Determinamos de que marca se trata\n",
    "    df_comp[\"is_iphone\"] = list(map(lambda x: 1 if x==\"iPhone\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_samsung\"] = list(map(lambda x: 1 if x==\"Samsung\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_motorola\"] = list(map(lambda x: 1 if x==\"Motorola\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_lg\"] = list(map(lambda x: 1 if x==\"LG\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_sony\"] = list(map(lambda x: 1 if x==\"Sony\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_other\"] = list(map(lambda x: 1 if (x!=\"Sony\" and x!=\"LG\" and x!=\"Motorola\" and x!=\"Samsung\" and x!=\"iPhone\") else 0, df_comp[\"marca\"]))\n",
    "    \n",
    "    # determinamos de que modelo se trata\n",
    "    pop_models = ['6','5s','6S','Galaxy J5','7','7 Plus','Galaxy S8','Galaxy S7','Galaxy S7 Edge','Galaxy J7 Prime','Moto G2 3G Dual','Galaxy S6 Edge','Galaxy S6 Flat','5c','6S Plus','Galaxy J7','6 Plus','Moto G4 Plus','SE','Galaxy Gran Prime Duos TV','4S','Galaxy S8 Plus','5','Moto G3 4G','Galaxy A5 2017','Moto X Play 4G Dual','Moto G5 Plus','Galaxy A7 2017','Moto X2','Galaxy A5']\n",
    "    for model in pop_models:\n",
    "        df_comp = is_object(df_comp,\"modelo\",model)\n",
    "\n",
    "    df_comp[\"is_other_model\"] = list(map(lambda x: 1 if x not in pop_models else 0, df_comp[\"modelo\"]))\n",
    "    \n",
    "    # Eliminamos las columnas con strings\n",
    "    df_comp.drop([\"model\",\"marca\",\"modelo\"],axis=1,inplace=True)\n",
    "    \n",
    "    return df_comp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_completitud(train, test, feature_columns, metodo, best_depth):\n",
    "    dtree = DecisionTreeClassifier(max_depth=5, criterion='gini') \n",
    "    completitud_precision_tamanio = {\"completitud\":[], \"precision\":[], \"tamanio\":[]}\n",
    "    for p in np.arange(0.0,.85,.05):   \n",
    "        df_train_p = preprocess(train,metodo,p)\n",
    "        scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n",
    "                            df_train_p[feature_columns].values, # X_train features\n",
    "                            df_train_p[\"target\"].values, # Y_train targets\n",
    "                            scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                            return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                            return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                            cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                            n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                           )\n",
    "        best_dtree_index = np.argmax(scores[\"test_roc_auc\"])\n",
    "        best_btree = scores[\"estimator\"][best_dtree_index]\n",
    "\n",
    "        y_esperado = test.target.values\n",
    "        y_pred = best_btree.predict(test[feature_columns].values)\n",
    "\n",
    "        accuracy = accuracy_score(y_esperado, y_pred)\n",
    "        tamanio = best_btree.tree_.node_count\n",
    "\n",
    "        completitud_precision_tamanio[\"completitud\"].append(100.0*p)\n",
    "        completitud_precision_tamanio[\"precision\"].append(accuracy)\n",
    "        completitud_precision_tamanio[\"tamanio\"].append(tamanio)\n",
    "        \n",
    "    return completitud_precision_tamanio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos los datasets para construir el dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first = pd.read_csv('../../data_aa/first_part_dataset.csv')\n",
    "\n",
    "df_second = pd.read_csv('../../data_aa/second_part_dataset.csv')\n",
    "\n",
    "df_thrid = pd.read_csv('../../data_aa/thrid_part_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first = df_first.rename(columns={'person_id': 'person'})\n",
    "\n",
    "df_first = df_first.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'person', 'diff_time_days_minnone',\n",
       "       'diff_time_daysnone', 'diff_time_dayscheckout',\n",
       "       'diff_time_daysconversion', 'diff_time_daysbrand listing',\n",
       "       'diff_time_dayssearch engine hit', 'diff_time_start_end', 'model',\n",
       "       'number_visits_model'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_second.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second = df_second.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'person', 'COUNT(users_logs)', 'avg_time_events'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_thrid.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thrid = df_thrid.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_first, df_second, on='person', how='left')\n",
    "\n",
    "df_final = pd.merge(df_final, df_thrid, on='person', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19414,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.target.isin([1.0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significado de las columnas:\n",
    "\n",
    " - Las columnas 'Android', 'BlackBerry', 'Chrome', 'FreeBSD', 'Linux', 'Mac', 'Tizen', 'Ubuntu', 'Unknown', 'Windows','iOS' indican la cantidad de veces que cada usuario ingreso a la pagina de Trocafone utilizando uno de esos esos sistemas operativos. Por ejemplo, si el valor de 'Android' para un usuario es 2 siginfica que ese usuario entro a la pagina dos veces desde un dispositivo con un sistema operativo Android\n",
    " \n",
    " - Las columnas 'Computer', 'Smartphone', 'Tablet' indican la cantidad de veces que cada usuario ingreso a la pagina de Trocafone utilizando uno de esos dispositivos\n",
    " \n",
    " - Las columnas 'ad campaign hit', 'brand listing', 'checkout', 'conversion', 'generic listing', 'lead','search engine hit', 'searched products', 'staticpage','viewed product', 'visited site' indican la cantidad de veces que cada usuario realizo cada uno de esos eventos\n",
    " \n",
    " - La columna 'diff_time_days_minnone' indica la cantidad de tiempo que paso desde el primer evento que realizo el usuario hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_daysnone' indica la cantidad de tiempo que paso desde el ultimo evento que el usuario realizo en la pagina hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_dayscheckout' indica la cantidad de tiempo que paso desde que el usuario realizo el ultimo checkout hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_daysconversion' indica la cantidad de tiempo que paso desde que el usuario realizo la ultima conversion hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_daysbrand listing' indica la cantidad de tiempo que paso desde que el usuario realizo la ultima brand listing hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_dayssearch engine hit' indica la cantidad de tiempo que paso desde que el usuario realizo el ultimo engine hit hasta el 2018/06/01\n",
    " \n",
    " - La columna 'diff_time_start_end' indica la cantidad de tiempo que paso entre el primer evento que realizo el usuario en la pagina y el ultimo\n",
    " \n",
    " - La columna 'model' indica el modelo de celuar que el usuario mas veces visito\n",
    " \n",
    " - La columna 'number_visits_model' indica cuantas veces el usuario visito el celular que mas visito\n",
    " \n",
    " - La columna 'COUNT(users_logs)' indica la cantidad de eventos que el usuario realizo en la pagina\n",
    " \n",
    " - La columna 'avg_time_events' indica el tiempo promedio que paso entre los eventos realizados por cada usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminamos las columnas con muy pocas variables o alta corrleación (Ej. Windows ~ Computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop([\"BlackBerry\",\"Chrome\",\"FreeBSD\",\"Linux\",\"Mac\",\"Other\",\"Tizen\",\"Ubuntu\",\"Unknown\",\"Windows\",\"person\"],\n",
    "              axis=1,\n",
    "              inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformamos algunas columnas para que su distribución se ajuste a una Chi cuadrado o Normal. Luego discretizamos por cuantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.loc[:,\"diff_time_daysnone\"] = np.log10(df_final[\"diff_time_daysnone\"]+1.)\n",
    "df_final.loc[:,\"diff_time_days_minnone\"] = np.log10(df_final.diff_time_days_minnone+1.)\n",
    "df_final.loc[:,\"diff_time_dayssearch engine hit\"] = np.log10(df_final[\"diff_time_dayssearch engine hit\"]+1.0)\n",
    "df_final.loc[:,\"COUNT(users_logs)\"] = np.log10(df_final[\"COUNT(users_logs)\"]+1.)\n",
    "df_final.loc[:,\"number_visits_model\"] = np.log10(1.+df_final.number_visits_model)\n",
    "df_final.loc[:,\"diff_time_daysbrand listing\"] = np.log10(01.+df_final[\"diff_time_daysbrand listing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         NaN\n",
       "1                    iPhone 7\n",
       "2                   iPhone 6S\n",
       "3    Motorola Moto G2 3G Dual\n",
       "4      Samsung Galaxy S8 Plus\n",
       "Name: model, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['model'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En el siguiente metodo  se realiza todo el preprocess de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo = preprocess(df_final.copy(),\"moda\",1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a seleccionar solo 980 casos que no realizaron una compra para balancear el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    }
   ],
   "source": [
    "df_completo_reduced_1 = df_completo[df_completo['target']==1]\n",
    "print(len(df_completo_reduced_1.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    }
   ],
   "source": [
    "df_completo_reduced_0 = df_completo[df_completo['target']==0].sample(n=980, random_state=1)\n",
    "print(len(df_completo_reduced_0.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_completo_reduced_1, df_completo_reduced_0])\n",
    "del df_completo_reduced_0, df_completo_reduced_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nos quedamos con aquellas columnas que no sean target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = list(set(df_train.columns.tolist())-set([\"target\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenemos el dataset de test y lo eliminamos del dataset de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_train.sample(frac=0.2, random_state=2)\n",
    "df_train.drop(df_test.index,axis=0,inplace=True)\n",
    "train_indexes = df_train.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificamos el balance de categorías en cada datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = df_train.shape[0]*1.\n",
    "N_test = df_test.shape[0]*1.\n",
    "\n",
    "N_train_1 = df_train.target.sum()\n",
    "N_train_0 = df_train.shape[0]*1.-N_train_1\n",
    "\n",
    "N_test_1 = df_test.target.sum()\n",
    "N_test_0 = df_test.shape[0]*1.-N_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de casos positivos en el train: %50.255102040816325\n",
      "Porcentaje de casos positivos en el test: %48.97959183673469\n"
     ]
    }
   ],
   "source": [
    "print(\"Porcentaje de casos positivos en el train: %\" + str(100.0*N_train_1/N_train))\n",
    "print(\"Porcentaje de casos positivos en el test: %\" + str(100.0*N_test_1/N_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando únicamente el dataset de train ejecutamos 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy','roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(max_depth=3, criterion='gini') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n",
    "                        df_train[feature_columns].values, # X_train features\n",
    "                        df_train[\"target\"].values, # Y_train targets\n",
    "                        scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                        return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                        return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                        cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                        n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los resultados en un DF de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5_fold_resultados = pd.DataFrame(scores)\n",
    "cv_5_fold_resultados.drop([\"fit_time\",\"score_time\",\"estimator\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.767145</td>\n",
       "      <td>0.791728</td>\n",
       "      <td>0.828612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.767516</td>\n",
       "      <td>0.763955</td>\n",
       "      <td>0.836356</td>\n",
       "      <td>0.824302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.779904</td>\n",
       "      <td>0.753185</td>\n",
       "      <td>0.842019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.760383</td>\n",
       "      <td>0.768127</td>\n",
       "      <td>0.802344</td>\n",
       "      <td>0.833754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.766773</td>\n",
       "      <td>0.764143</td>\n",
       "      <td>0.812388</td>\n",
       "      <td>0.830726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "0       0.732484        0.767145      0.791728       0.828612\n",
       "1       0.767516        0.763955      0.836356       0.824302\n",
       "2       0.707006        0.779904      0.753185       0.842019\n",
       "3       0.760383        0.768127      0.802344       0.833754\n",
       "4       0.766773        0.764143      0.812388       0.830726"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculamos los estadisticos de cada experimento, tanto para train como para test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.746833</td>\n",
       "      <td>0.768655</td>\n",
       "      <td>0.799200</td>\n",
       "      <td>0.831883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.026463</td>\n",
       "      <td>0.006549</td>\n",
       "      <td>0.030568</td>\n",
       "      <td>0.006629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.763955</td>\n",
       "      <td>0.753185</td>\n",
       "      <td>0.824302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.764143</td>\n",
       "      <td>0.791728</td>\n",
       "      <td>0.828612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.760383</td>\n",
       "      <td>0.767145</td>\n",
       "      <td>0.802344</td>\n",
       "      <td>0.830726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.766773</td>\n",
       "      <td>0.768127</td>\n",
       "      <td>0.812388</td>\n",
       "      <td>0.833754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.767516</td>\n",
       "      <td>0.779904</td>\n",
       "      <td>0.836356</td>\n",
       "      <td>0.842019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "count       5.000000        5.000000      5.000000       5.000000\n",
       "mean        0.746833        0.768655      0.799200       0.831883\n",
       "std         0.026463        0.006549      0.030568       0.006629\n",
       "min         0.707006        0.763955      0.753185       0.824302\n",
       "25%         0.732484        0.764143      0.791728       0.828612\n",
       "50%         0.760383        0.767145      0.802344       0.830726\n",
       "75%         0.766773        0.768127      0.812388       0.833754\n",
       "max         0.767516        0.779904      0.836356       0.842019"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionamos el árbol que mejor accuracy haya tenido durante el train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dtree_index = np.argmax(scores[\"test_accuracy\"]*scores[\"test_roc_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_btree = scores[\"estimator\"][best_dtree_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluamos su accuracy sobre el 20% restante del dataset que no formó parte del 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_esperado = df_test.target.values\n",
    "y_pred = best_btree.predict(df_test[feature_columns].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score validation: 0.7831632653061225\n"
     ]
    }
   ],
   "source": [
    "print(\"Score validation: \" + str(accuracy_score(y_esperado, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot_data = StringIO()\n",
    "# export_graphviz(best_btree, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True)\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volvemos a correr el 5-fold CV pero completado el dataset de train con un dado porcentaje de completitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_80 = preprocess(df_final.loc[train_indexes].copy(),\"moda\",.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1568, 28)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.loc[train_indexes].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n",
    "                        df_train_80[feature_columns].values, # X_train features\n",
    "                        df_train_80[\"target\"].values, # Y_train targets\n",
    "                        scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                        return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                        return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                        cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                        n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5_fold_resultados = pd.DataFrame(scores)\n",
    "cv_5_fold_resultados.drop([\"fit_time\",\"score_time\",\"estimator\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.655925</td>\n",
       "      <td>0.806379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.615135</td>\n",
       "      <td>0.835405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.793522</td>\n",
       "      <td>0.675556</td>\n",
       "      <td>0.850739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.848535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.740891</td>\n",
       "      <td>0.676111</td>\n",
       "      <td>0.838295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "0       0.682540        0.714286      0.655925       0.806379\n",
       "1       0.612903        0.772358      0.615135       0.835405\n",
       "2       0.639344        0.793522      0.675556       0.850739\n",
       "3       0.688525        0.789474      0.730000       0.848535\n",
       "4       0.622951        0.740891      0.676111       0.838295"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.649253</td>\n",
       "      <td>0.762106</td>\n",
       "      <td>0.670545</td>\n",
       "      <td>0.835871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.034502</td>\n",
       "      <td>0.033830</td>\n",
       "      <td>0.041458</td>\n",
       "      <td>0.017729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.615135</td>\n",
       "      <td>0.806379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.740891</td>\n",
       "      <td>0.655925</td>\n",
       "      <td>0.835405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>0.675556</td>\n",
       "      <td>0.838295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.676111</td>\n",
       "      <td>0.848535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.793522</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>0.850739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "count       5.000000        5.000000      5.000000       5.000000\n",
       "mean        0.649253        0.762106      0.670545       0.835871\n",
       "std         0.034502        0.033830      0.041458       0.017729\n",
       "min         0.612903        0.714286      0.615135       0.806379\n",
       "25%         0.622951        0.740891      0.655925       0.835405\n",
       "50%         0.639344        0.772358      0.675556       0.838295\n",
       "75%         0.682540        0.789474      0.676111       0.848535\n",
       "max         0.688525        0.793522      0.730000       0.850739"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dtree_index = np.argmax(scores[\"test_accuracy\"]*scores[\"test_roc_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_btree = scores[\"estimator\"][best_dtree_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_esperado = df_test.target.values\n",
    "y_pred = best_btree.predict(df_test[feature_columns].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score validation: 0.6045918367346939\n"
     ]
    }
   ],
   "source": [
    "print(\"Score validation: \" + str(accuracy_score(y_esperado, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot_data = StringIO()\n",
    "# export_graphviz(best_btree, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True)\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis de completitud vs tamanio del arbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_moda = test_completitud(train=df_final.loc[train_indexes].copy(),\n",
    "                 test=df_test.copy(),\n",
    "                 feature_columns=feature_columns,\n",
    "                 metodo=\"moda\",\n",
    "                 best_depth=5\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-5ddd01a10e88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                  \u001b[0mfeature_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                  \u001b[0mmetodo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"moda_por_clase\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                  \u001b[0mbest_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                 )\n",
      "\u001b[1;32m<ipython-input-8-7a9a53c7857d>\u001b[0m in \u001b[0;36mtest_completitud\u001b[1;34m(train, test, feature_columns, metodo, best_depth)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mcompletitud_precision_tamanio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"completitud\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"precision\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tamanio\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m.85\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdf_train_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetodo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n\u001b[0;32m      7\u001b[0m                             \u001b[0mdf_train_p\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# X_train features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-793aef03ea3d>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(df, metodo, rate_to_complete)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Los completamos con un rate_to_complete y utilizando un determinado metodo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdf_comp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomplete_nans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcols_in_null\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrate_to_complete\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetodo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Discretizamos las columnas que ya sabemos que tienen una distribucion dada (Chi, Normal)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ab38619e9250>\u001b[0m in \u001b[0;36mcomplete_nans\u001b[1;34m(df, null_cols, rate_to_complete, method)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnull_cols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mdf0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_and_replace_nans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrate_to_complete\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_and_replace_nans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrate_to_complete\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-ab38619e9250>\u001b[0m in \u001b[0;36mfind_and_replace_nans\u001b[1;34m(df, col, rate_to_complete)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_and_replace_nans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrate_to_complete\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mindex_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrate_to_complete\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[0;32m   4199\u001b[0m                              \"provide positive value.\")\n\u001b[0;32m   4200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4201\u001b[1;33m         \u001b[0mlocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4202\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_copy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: a must be greater than 0"
     ]
    }
   ],
   "source": [
    "test_moda_por_clase = test_completitud(train=df_final.loc[train_indexes].copy(),\n",
    "                 test=df_test.copy(),\n",
    "                 feature_columns=feature_columns,\n",
    "                 metodo=\"moda_por_clase\",\n",
    "                 best_depth=5\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(test_moda[\"completitud\"],test_moda[\"tamanio\"], linewidth=3.0)\n",
    "ax1.set_xlabel('Porcentaje de completitud')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Cantidad de nodos del arbol')\n",
    "plt.title(\"Imputacion de datos faltantes por moda\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(test_moda_por_clase[\"completitud\"],test_moda_por_clase[\"tamanio\"], linewidth=3.0)\n",
    "ax1.set_xlabel('Porcentaje de completitud')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Cantidad de nodos del arbol')\n",
    "plt.title(\"Imputacion de datos faltantes por moda\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Tolerancia al ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) y b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creamos una funcion que genera un array de un largo determinado con valores 0 y un porcentage de valores random. Los valores random que se generen van a depender de la media de la columna para la cual quiere generarse ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array_noise(elements_in_column, percentage_noise, mean_column):\n",
    "    \n",
    "    number_noise = round(elements_in_column*percentage_noise)\n",
    "    \n",
    "    mu, sigma = 0, mean_column\n",
    "    \n",
    "    noise = np.random.normal(mu, sigma, [number_noise,1])\n",
    "    \n",
    "    array_noise = noise.tolist()\n",
    "    \n",
    "    array_noise_final = []\n",
    "    \n",
    "    for item in array_noise:\n",
    "        \n",
    "        array_noise_final.append(item[0])\n",
    "    \n",
    "    array_no_noise = [0]*(elements_in_column - number_noise)\n",
    "    \n",
    "    noise = array_noise_final + array_no_noise\n",
    "    \n",
    "    random.shuffle(noise)\n",
    "    \n",
    "    return noise    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creamos una funcion que, utilizando la funcion anterior, agrega un % de ruido al atributo que indiquemos de un dataset determinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_with_noise(df, name_attribute, percentage_noise):\n",
    "    \n",
    "    dataframe = df.copy()\n",
    "    \n",
    "    number_observations = len(dataframe[name_attribute].values)\n",
    "    \n",
    "    mean_column_attribute = dataframe[name_attribute].mean()\n",
    "    \n",
    "    array_noise = create_array_noise(number_observations, percentage_noise, mean_column_attribute)\n",
    "    \n",
    "    dataframe[name_attribute] = dataframe[name_attribute] + array_noise\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_familiy_datasets(df_train, df_test, name_attribute):\n",
    "    \n",
    "    # Modelo con la mejor combinacion de hiperparametros del punto 2.2\n",
    "    \n",
    "    dtree = DecisionTreeClassifier(max_depth=6, criterion='gini')\n",
    "    \n",
    "    noise_accuracy_tamanio = {\"noise\":[], \"accuracy\":[], \"tamanio\":[]}\n",
    "    \n",
    "    # Niveles de ruido\n",
    "    \n",
    "    for noise in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35]:\n",
    "        \n",
    "        # Agregamos el ruido al attribute elegido\n",
    "        \n",
    "        df_noise = generate_dataset_with_noise(df_train, name_attribute, noise)\n",
    "        \n",
    "        scoring = ['accuracy']\n",
    "        \n",
    "        scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n",
    "                            df_noise.drop(columns=['target']).values, # X_train features\n",
    "                            df_noise[\"target\"].values, # Y_train targets\n",
    "                            scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                            return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                            return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                            cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                            n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                           )\n",
    "        \n",
    "        # Seleccionamos el mejor modelo\n",
    "        \n",
    "        best_dtree_index = np.argmax(scores[\"test_accuracy\"])\n",
    "        best_btree = scores[\"estimator\"][best_dtree_index]\n",
    "\n",
    "        y_esperado = df_test['target'].values\n",
    "        y_pred = best_btree.predict(df_test.drop(columns=['target']).values)\n",
    "\n",
    "        accuracy = accuracy_score(y_esperado, y_pred)\n",
    "        tamanio = best_btree.tree_.node_count\n",
    "\n",
    "        noise_accuracy_tamanio[\"noise\"].append(100.0*noise)\n",
    "        noise_accuracy_tamanio[\"accuracy\"].append(accuracy)\n",
    "        noise_accuracy_tamanio[\"tamanio\"].append(tamanio)\n",
    "        \n",
    "    return noise_accuracy_tamanio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_2_4 = run_model_familiy_datasets(df_train, df_test, 'diff_time_start_end')\n",
    "\n",
    "df_results_2_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(df_results_2_4[\"noise\"], df_results_2_4[\"tamanio\"], linewidth=3.0)\n",
    "ax1.set_xlabel('Porcentaje de noise')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Cantidad de nodos del arbol')\n",
    "plt.title(\"Imputacion de datos faltantes por moda\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(df_results_2_4[\"noise\"], df_results_2_4[\"accuracy\"], linewidth=3.0)\n",
    "ax1.set_xlabel('Porcentaje de noise')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Cantidad de nodos del arbol')\n",
    "plt.title(\"Imputacion de datos faltantes por moda\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La performance del modelo no parece verse afectada por el noise. Puede ser porque no es una varaible importante para el mismo?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Graficamos el tamaño del arbol (cantidad de nodos) en funcion del ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Graficamos la performance (accuracy) en funcion del ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Analizamos los resultados: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el modelo con los datos de train y predecimos sobre test. Los resultamos que vamos a obtener son las probabilidades que el modelo asigna para cada clase posible (recordemos que en este caso son solo 2: compro o no compro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "\n",
    "scoring = ['accuracy','roc_auc']\n",
    "\n",
    "scores_nb = cross_validate(clf, # Instancia de árbol a entrenar en cada fold\n",
    "                        df_train.drop(columns=['target']).values, # X_train features\n",
    "                        df_train[\"target\"].values, # Y_train targets\n",
    "                        scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                        return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                        return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                        cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                        n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                       )\n",
    "scores_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5_fold_resultados_naive_bayes = pd.DataFrame(scores_nb)\n",
    "\n",
    "cv_5_fold_resultados_naive_bayes.drop([\"fit_time\",\"score_time\",\"estimator\"],axis=1,inplace=True)\n",
    "\n",
    "cv_5_fold_resultados_naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5_fold_resultados_naive_bayes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_naive_bayes_index = np.argmax(scores_nb[\"test_accuracy\"]*scores_nb[\"test_roc_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_naive_bayes = scores[\"estimator\"][best_naive_bayes_index]\n",
    "\n",
    "y_esperado_nb = df_test.target.values\n",
    "\n",
    "y_pred_nb = best_naive_bayes.predict(df_test[feature_columns].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy del modelo naive bayes sobre el dataset de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score validation: \" + str(accuracy_score(y_esperado_nb, y_pred_nb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenemos las probabilidades de pertenecia a cada clase que el modelo calcula para cada observacion de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_probabilities(model, dataset):\n",
    "    \n",
    "    y_pred = model.predict_proba(dataset.drop(columns=['target']))\n",
    "    \n",
    "    array_pred_class_0 = []\n",
    "    \n",
    "    array_pred_class_1 = []\n",
    "    \n",
    "    for prediction in y_pred:\n",
    "        \n",
    "        array_pred_class_0.append('{:f}'.format(prediction[0]))\n",
    "        \n",
    "        array_pred_class_1.append('{:f}'.format(prediction[1]))\n",
    "    \n",
    "    d = {'proba_class_0': array_pred_class_0, 'proba_class_1': array_pred_class_1}\n",
    "    \n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    return df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_probabilities(best_naive_bayes, df_test).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenemos las probabilidades condicionales para un atributo del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_probabilities_attribute(dataframe, name_attribute):\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    df['target_no'] = np.where(df['target'] == 0, 1, 0)\n",
    "    \n",
    "    d = {'target': ['sum'], 'target_no': ['sum']}\n",
    "\n",
    "    df_cond = df.groupby([name_attribute]).agg(d)\n",
    "\n",
    "    df_cond.columns = ['_'.join(col) for col in df_cond.columns.values]\n",
    "\n",
    "    df_cond = df_cond.reset_index()\n",
    "\n",
    "    df_cond['target_yes'] = round(df_cond['target_sum']/df_cond['target_sum'].sum(),3)\n",
    "    \n",
    "    df_cond['target_no'] = round(df_cond['target_no_sum']/df_cond['target_no_sum'].sum(),3)\n",
    "    \n",
    "    df_cond = df_cond.drop(columns=['target_sum', 'target_no_sum'])\n",
    "    \n",
    "    return df_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Por ejemplo, dado el valor de 1 para target, la probabilidad de que 'Android' tome el valor de 0 es de 0,594 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = get_conditional_probabilities_attribute(df_train, 'Android')\n",
    "\n",
    "print(df_test.head())\n",
    "\n",
    "df_test['target_yes'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
