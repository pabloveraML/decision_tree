{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos todas las bibliotecas necesarias para ejecutar todos los métodos del jupyter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es buena costumbre cargarlos al principio, asi no se estar cargando en memoria repetidas veces cada paquete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "#import pydotplus\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "plt.rcParams['figure.figsize'] = [30, 10]\n",
    "plt.rcParams['font.size'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de funciones auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este método se aplicará la discretización de una determinada columna en función de los cuantiles.\n",
    "La lógica es:\n",
    "    - Si el valor x de la columna _column_ cae por debajo del cuantil q1 entonces x -> [1,0,0,0]\n",
    "    - Si el valor x de la columna _column_ esta entre q1 y q2 entonces x -> [0,1,0,0]\n",
    "    - Si el valor x de la columna _column_ esta entre q2 y q3 entonces x -> [0,0,1,0]\n",
    "    - Si el valor x de la columna _column_ es mayor a q3 entonces x -> [0,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(df, column):\n",
    "    q1 = df_final[column].quantile(.25)\n",
    "    q2 = df_final[column].quantile(.5)\n",
    "    q3 = df_final[column].quantile(.75)\n",
    "    df[column+\"_in_q1\"] = 0\n",
    "    df[column+\"_in_q2\"] = 0\n",
    "    df[column+\"_in_q3\"] = 0\n",
    "    df[column+\"_in_q4\"] = 0\n",
    "    df[column+\"_in_q1\"] = list(map(lambda x: 1 if x <=q1 else 0, df[column]))\n",
    "    df[column+\"_in_q2\"] = list(map(lambda x: 1 if (q1 < x and x <=q2) else 0, df[column]))\n",
    "    df[column+\"_in_q3\"] = list(map(lambda x: 1 if (q2 < x and x <=q3) else 0, df[column]))\n",
    "    df[column+\"_in_q4\"] = list(map(lambda x: 1 if x>q3 else 0, df[column]))\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este metodo determinamos si el objeto que se encuentra en la columna _column_ es igual a un _object name_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_object(df,column,object_name):\n",
    "    df[\"is_\"+str(object_name)] = list(map(lambda x: 1 if x == object_name else 0, df[column]))\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante los siguientes dos métodos podemos completar los datos faltantes dentro de un DataFrame. Para ello se necesita especificar:\n",
    "    - Un DataFrame: df\n",
    "    - Las columnas que contienen NaNs: null_cols\n",
    "    - El ratio de completitud que se desea: rate_to_complete\n",
    "    - El metodo con el que se desea completar los NaNs: method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_replace_nans(df,col,rate_to_complete):\n",
    "    if df[col][df[col].isnull()].shape[0]>0:\n",
    "        index_list = df[col][df[col].isnull()].sample(frac=rate_to_complete,random_state=1).index.tolist()\n",
    "\n",
    "        if len(index_list)>0:\n",
    "            moda = df[(~df[col].isnull())][col].mode()[0]\n",
    "            df.loc[index_list,col] = moda\n",
    "    return df\n",
    "    \n",
    "def complete_nans(df,null_cols,rate_to_complete,method):\n",
    "    if method==\"moda_por_clase\":\n",
    "        for col in null_cols:\n",
    "            df0 = find_and_replace_nans(df[df.target==0.0],col,rate_to_complete)\n",
    "            df1 = find_and_replace_nans(df[df.target==1.0],col,rate_to_complete)\n",
    "            df = pd.concat([df0,df1])\n",
    "    else:\n",
    "        for col in null_cols:\n",
    "            df = find_and_replace_nans(df,col,rate_to_complete)\n",
    "    \n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el metodo _preprocess_ replicamos todos los pasos necesarios para preprocesar la data original. De esta forma podemos realizar la completitud de los datos pasandole un *rate_to_complete*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, metodo, rate_to_complete):\n",
    "    \n",
    "    # Extraemos las filas de la columna 'model' que tienen valores NaNs y luego los completamos con un valor por defecto\n",
    "    \n",
    "    #model_null_indexes = df[df.model.isnull()].index.tolist()\n",
    "    #df.loc[model_null_indexes,\"model\"] = \"other other\"\n",
    "    \n",
    "    df['model'] = df['model'].fillna('other other')\n",
    "    \n",
    "    # Obtenemos las columnas que tienen al menos un NaN\n",
    "    cols_in_null = df.columns[df.isnull().sum(axis=0)>0].tolist()\n",
    "    \n",
    "    # Los completamos con un rate_to_complete y utilizando un determinado metodo\n",
    "    df_comp = complete_nans(df,cols_in_null,rate_to_complete,metodo)\n",
    "    \n",
    "    # Discretizamos las columnas que ya sabemos que tienen una distribucion dada (Chi, Normal)\n",
    "    # (Se usan cuantiles para ver en qué segmento caen)\n",
    "    for column in [\"diff_time_days_minnone\",\"diff_time_dayssearch engine hit\",\"COUNT(users_logs)\",\"number_visits_model\",\"diff_time_daysbrand listing\",\"diff_time_daysnone\",'viewed product', 'visited site', 'staticpage']:\n",
    "        df_comp = discretize(df_comp,column)\n",
    "    \n",
    "    # De la columna 'model' extraemos marca y modelo del celular visitado o  comprado\n",
    "    df_comp[\"marca\"] = df_comp.model.str.split(\" \",1,expand=True)[0]\n",
    "    df_comp[\"modelo\"] = df_comp.model.str.split(\" \",1,expand=True)[1]\n",
    "    \n",
    "    # Determinamos de que marca se trata\n",
    "    df_comp[\"is_iphone\"] = list(map(lambda x: 1 if x==\"iPhone\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_samsung\"] = list(map(lambda x: 1 if x==\"Samsung\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_motorola\"] = list(map(lambda x: 1 if x==\"Motorola\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_lg\"] = list(map(lambda x: 1 if x==\"LG\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_sony\"] = list(map(lambda x: 1 if x==\"Sony\" else 0, df_comp[\"marca\"]))\n",
    "    df_comp[\"is_other\"] = list(map(lambda x: 1 if (x!=\"Sony\" and x!=\"LG\" and x!=\"Motorola\" and x!=\"Samsung\" and x!=\"iPhone\") else 0, df_comp[\"marca\"]))\n",
    "    \n",
    "    # determinamos de que modelo se trata\n",
    "    pop_models = ['6','5s','6S','Galaxy J5','7','7 Plus','Galaxy S8','Galaxy S7','Galaxy S7 Edge','Galaxy J7 Prime','Moto G2 3G Dual','Galaxy S6 Edge','Galaxy S6 Flat','5c','6S Plus','Galaxy J7','6 Plus','Moto G4 Plus','SE','Galaxy Gran Prime Duos TV','4S','Galaxy S8 Plus','5','Moto G3 4G','Galaxy A5 2017','Moto X Play 4G Dual','Moto G5 Plus','Galaxy A7 2017','Moto X2','Galaxy A5']\n",
    "    for model in pop_models:\n",
    "        df_comp = is_object(df_comp,\"modelo\",model)\n",
    "\n",
    "    df_comp[\"is_other_model\"] = list(map(lambda x: 1 if x not in pop_models else 0, df_comp[\"modelo\"]))\n",
    "    \n",
    "    # Eliminamos las columnas con strings\n",
    "    df_comp.drop([\"model\",\"marca\",\"modelo\"],axis=1,inplace=True)\n",
    "    \n",
    "    return df_comp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante el método *test_completitud* almacenamos los resultados del 5-fold CV para distintos niveles de completitud: yendo desde un 0% a un 80% de completitud, variando a 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_completitud(train, test, feature_columns, metodo, best_depth):\n",
    "    dtree = DecisionTreeClassifier(max_depth=5, criterion='gini') \n",
    "    completitud_precision_tamanio = {\"completitud\":[], \"precision\":[], \"tamanio\":[]}\n",
    "    for p in np.arange(0.0,.85,.05):   \n",
    "        df_train_p = preprocess(train,metodo,p)\n",
    "        scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n",
    "                            df_train_p[feature_columns].values, # X_train features\n",
    "                            df_train_p[\"target\"].values, # Y_train targets\n",
    "                            scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                            return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                            return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                            cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                            n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                           )\n",
    "        best_dtree_index = np.argmax(scores[\"test_roc_auc\"])\n",
    "        best_btree = scores[\"estimator\"][best_dtree_index]\n",
    "\n",
    "        y_esperado = test.target.values\n",
    "        y_pred = best_btree.predict(test[feature_columns].values)\n",
    "\n",
    "        accuracy = accuracy_score(y_esperado, y_pred)\n",
    "        tamanio = best_btree.tree_.node_count\n",
    "\n",
    "        completitud_precision_tamanio[\"completitud\"].append(100.0*p)\n",
    "        completitud_precision_tamanio[\"precision\"].append(accuracy)\n",
    "        completitud_precision_tamanio[\"tamanio\"].append(tamanio)\n",
    "        \n",
    "    return completitud_precision_tamanio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datasets para construir el dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first = pd.read_csv('../../data_aa/first_part_dataset.csv')\n",
    "\n",
    "df_second = pd.read_csv('../../data_aa/second_part_dataset.csv')\n",
    "\n",
    "df_thrid = pd.read_csv('../../data_aa/thrid_part_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first = df_first.rename(columns={'person_id': 'person'})\n",
    "\n",
    "df_first = df_first.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'person', 'diff_time_days_minnone',\n",
       "       'diff_time_daysnone', 'diff_time_dayscheckout',\n",
       "       'diff_time_daysconversion', 'diff_time_daysbrand listing',\n",
       "       'diff_time_dayssearch engine hit', 'diff_time_start_end', 'model',\n",
       "       'number_visits_model'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_second.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_second = df_second.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'person', 'COUNT(users_logs)', 'avg_time_events'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_thrid.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thrid = df_thrid.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_first, df_second, on='person', how='left')\n",
    "\n",
    "df_final = pd.merge(df_final, df_thrid, on='person', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19414,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.target.isin([1.0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significado de las columnas:\n",
    "\n",
    " ##### - Las columnas 'Android', 'BlackBerry', 'Chrome', 'FreeBSD', 'Linux', 'Mac', 'Tizen', 'Ubuntu', 'Unknown', 'Windows','iOS' indican la cantidad de veces que cada usuario ingreso a la pagina de Trocafone utilizando uno de esos esos sistemas operativos. Por ejemplo, si el valor de 'Android' para un usuario es 2 siginfica que ese usuario entro a la pagina dos veces desde un dispositivo con un sistema operativo Android\n",
    " \n",
    "#####  - Las columnas 'Computer', 'Smartphone', 'Tablet' indican la cantidad de veces que cada usuario ingreso a la pagina de Trocafone utilizando uno de esos dispositivos\n",
    " \n",
    "#####  - Las columnas 'ad campaign hit', 'brand listing', 'checkout', 'conversion', 'generic listing', 'lead','search engine hit', 'searched products', 'staticpage','viewed product', 'visited site' indican la cantidad de veces que cada usuario realizo cada uno de esos eventos\n",
    " \n",
    "#####  - La columna 'diff_time_days_minnone' indica la cantidad de tiempo que paso desde el primer evento que realizo el usuario hasta el 2018/06/01\n",
    " \n",
    "#####  - La columna 'diff_time_daysnone' indica la cantidad de tiempo que paso desde el ultimo evento que el usuario realizo en la pagina hasta el 2018/06/01\n",
    " \n",
    "#####  - La columna 'diff_time_dayscheckout' indica la cantidad de tiempo que paso desde que el usuario realizo el ultimo checkout hasta el 2018/06/01\n",
    " \n",
    "#####  - La columna 'diff_time_daysconversion' indica la cantidad de tiempo que paso desde que el usuario realizo la ultima conversion hasta el 2018/06/01\n",
    " \n",
    "#####  - La columna 'diff_time_daysbrand listing' indica la cantidad de tiempo que paso desde que el usuario realizo la ultima brand listing hasta el 2018/06/01\n",
    " \n",
    "#####  - La columna 'diff_time_dayssearch engine hit' indica la cantidad de tiempo que paso desde que el usuario realizo el ultimo engine hit hasta el 2018/06/01\n",
    " \n",
    "#####  - La columna 'diff_time_start_end' indica la cantidad de tiempo que paso entre el primer evento que realizo el usuario en la pagina y el ultimo\n",
    " \n",
    "#####  - La columna 'model' indica el modelo de celular que el usuario mas veces visito\n",
    " \n",
    "#####  - La columna 'number_visits_model' indica cuantas veces el usuario visito el celular que mas visito\n",
    " \n",
    "#####  - La columna 'COUNT(users_logs)' indica la cantidad de eventos que el usuario realizo en la pagina\n",
    " \n",
    "#####  - La columna 'avg_time_events' indica el tiempo promedio que paso entre los eventos realizados por cada usuario\n",
    "\n",
    "##### - La columna 'Target' indica si el usuario realizo una conversion en la pagina durante el periodo que va desde la fecha 2018/06/01 hasta la 2018/06/15\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduccion de dimensionalidad del dataset\n",
    "Eliminamos las columnas con muy poca información (desvío casi nulo) o alta corrleación (Ej. Windows ~ Computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.drop([\"BlackBerry\",\"Chrome\",\"FreeBSD\",\"Linux\",\"Mac\",\"Other\",\"Tizen\",\"Ubuntu\",\"Unknown\",\"Windows\",\"person\"],\n",
    "              axis=1,\n",
    "              inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion de atributos de numericos a categoricos\n",
    "Transformamos algunas columnas para que su distribución se ajuste a una Chi cuadrado o Normal. Luego discretizamos por cuantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Android', 'Computer', 'Smartphone', 'Tablet', 'ad campaign hit',\n",
       "       'brand listing', 'checkout', 'conversion', 'generic listing',\n",
       "       'iOS', 'lead', 'search engine hit', 'searched products',\n",
       "       'staticpage', 'target', 'viewed product', 'visited site',\n",
       "       'diff_time_days_minnone', 'diff_time_daysnone',\n",
       "       'diff_time_dayscheckout', 'diff_time_daysconversion',\n",
       "       'diff_time_daysbrand listing', 'diff_time_dayssearch engine hit',\n",
       "       'diff_time_start_end', 'model', 'number_visits_model',\n",
       "       'COUNT(users_logs)', 'avg_time_events'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df_final.columns.values))\n",
    "\n",
    "df_final.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Android</th>\n",
       "      <th>Computer</th>\n",
       "      <th>Smartphone</th>\n",
       "      <th>Tablet</th>\n",
       "      <th>ad campaign hit</th>\n",
       "      <th>brand listing</th>\n",
       "      <th>checkout</th>\n",
       "      <th>conversion</th>\n",
       "      <th>generic listing</th>\n",
       "      <th>iOS</th>\n",
       "      <th>...</th>\n",
       "      <th>diff_time_daysnone</th>\n",
       "      <th>diff_time_dayscheckout</th>\n",
       "      <th>diff_time_daysconversion</th>\n",
       "      <th>diff_time_daysbrand listing</th>\n",
       "      <th>diff_time_dayssearch engine hit</th>\n",
       "      <th>diff_time_start_end</th>\n",
       "      <th>model</th>\n",
       "      <th>number_visits_model</th>\n",
       "      <th>COUNT(users_logs)</th>\n",
       "      <th>avg_time_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.3</td>\n",
       "      <td>14.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>0.033449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>iPhone 7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>iPhone 6S</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Android  Computer  Smartphone  Tablet  ad campaign hit  brand listing  \\\n",
       "0      0.0       2.0         0.0     0.0              0.0            0.0   \n",
       "1      1.0       0.0         1.0     0.0              1.0            0.0   \n",
       "2      1.0       0.0         1.0     0.0              5.0            0.0   \n",
       "\n",
       "   checkout  conversion  generic listing  iOS       ...         \\\n",
       "0       3.0         0.0              1.0  0.0       ...          \n",
       "1       1.0         0.0              1.0  0.0       ...          \n",
       "2       1.0         0.0              4.0  0.0       ...          \n",
       "\n",
       "   diff_time_daysnone  diff_time_dayscheckout  diff_time_daysconversion  \\\n",
       "0                14.3                    14.3                       NaN   \n",
       "1                 3.0                     3.0                       NaN   \n",
       "2                 0.3                     0.3                       NaN   \n",
       "\n",
       "   diff_time_daysbrand listing  diff_time_dayssearch engine hit  \\\n",
       "0                          NaN                              NaN   \n",
       "1                          NaN                              3.0   \n",
       "2                          NaN                              NaN   \n",
       "\n",
       "   diff_time_start_end      model  number_visits_model  COUNT(users_logs)  \\\n",
       "0                  0.2        NaN                  NaN                  6   \n",
       "1                  0.0   iPhone 7                  3.0                 17   \n",
       "2                  0.0  iPhone 6S                  2.0                 19   \n",
       "\n",
       "   avg_time_events  \n",
       "0         0.033449  \n",
       "1         0.000448  \n",
       "2         0.000415  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.loc[:,\"diff_time_daysnone\"] = np.log10(df_final[\"diff_time_daysnone\"]+1.)\n",
    "df_final.loc[:,\"diff_time_days_minnone\"] = np.log10(df_final.diff_time_days_minnone+1.)\n",
    "df_final.loc[:,\"diff_time_dayssearch engine hit\"] = np.log10(df_final[\"diff_time_dayssearch engine hit\"]+1.0)\n",
    "df_final.loc[:,\"COUNT(users_logs)\"] = np.log10(df_final[\"COUNT(users_logs)\"]+1.)\n",
    "df_final.loc[:,\"number_visits_model\"] = np.log10(1.+df_final.number_visits_model)\n",
    "df_final.loc[:,\"diff_time_daysbrand listing\"] = np.log10(01.+df_final[\"diff_time_daysbrand listing\"])\n",
    "\n",
    "df_final.loc[:,'viewed product'] = np.log10(01.+df_final['viewed product'])\n",
    "df_final.loc[:,'visited site'] = np.log10(01.+df_final['visited site'])\n",
    "df_final.loc[:,'staticpage'] = np.log10(01.+df_final['staticpage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a transformar las columnas Android, Computer, Smartphone y tablet de tal forma que sus valores van a ser solo 0 o 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Android'] = np.where(df_final['Android'] == 0, 0, 1)\n",
    "df_final['Computer'] = np.where(df_final['Computer'] == 0, 0, 1)\n",
    "df_final['Smartphone'] = np.where(df_final['Smartphone'] == 0, 0, 1)\n",
    "df_final['Tablet'] = np.where(df_final['Tablet'] == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         NaN\n",
       "1                    iPhone 7\n",
       "2                   iPhone 6S\n",
       "3    Motorola Moto G2 3G Dual\n",
       "4      Samsung Galaxy S8 Plus\n",
       "Name: model, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['model'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente metodo se realiza todo el preprocess de los datos. Ademas se completan todos los datos faltantes utilizando la moda de los datos que si estan completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo = preprocess(df_final.copy(),\"moda\",1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Android', 'Computer', 'Smartphone', 'Tablet', 'ad campaign hit',\n",
       "       'brand listing', 'checkout', 'conversion', 'generic listing',\n",
       "       'iOS', 'lead', 'search engine hit', 'searched products', 'target',\n",
       "       'diff_time_dayscheckout', 'diff_time_daysconversion',\n",
       "       'diff_time_start_end', 'avg_time_events',\n",
       "       'diff_time_days_minnone_in_q1', 'diff_time_days_minnone_in_q2',\n",
       "       'diff_time_days_minnone_in_q3', 'diff_time_days_minnone_in_q4',\n",
       "       'diff_time_dayssearch engine hit_in_q1',\n",
       "       'diff_time_dayssearch engine hit_in_q2',\n",
       "       'diff_time_dayssearch engine hit_in_q3',\n",
       "       'diff_time_dayssearch engine hit_in_q4', 'COUNT(users_logs)_in_q1',\n",
       "       'COUNT(users_logs)_in_q2', 'COUNT(users_logs)_in_q3',\n",
       "       'COUNT(users_logs)_in_q4', 'number_visits_model_in_q1',\n",
       "       'number_visits_model_in_q2', 'number_visits_model_in_q3',\n",
       "       'number_visits_model_in_q4', 'diff_time_daysbrand listing_in_q1',\n",
       "       'diff_time_daysbrand listing_in_q2',\n",
       "       'diff_time_daysbrand listing_in_q3',\n",
       "       'diff_time_daysbrand listing_in_q4', 'diff_time_daysnone_in_q1',\n",
       "       'diff_time_daysnone_in_q2', 'diff_time_daysnone_in_q3',\n",
       "       'diff_time_daysnone_in_q4', 'viewed product_in_q1',\n",
       "       'viewed product_in_q2', 'viewed product_in_q3',\n",
       "       'viewed product_in_q4', 'visited site_in_q1', 'visited site_in_q2',\n",
       "       'visited site_in_q3', 'visited site_in_q4', 'staticpage_in_q1',\n",
       "       'staticpage_in_q2', 'staticpage_in_q3', 'staticpage_in_q4',\n",
       "       'is_iphone', 'is_samsung', 'is_motorola', 'is_lg', 'is_sony',\n",
       "       'is_other', 'is_6', 'is_5s', 'is_6S', 'is_Galaxy J5', 'is_7',\n",
       "       'is_7 Plus', 'is_Galaxy S8', 'is_Galaxy S7', 'is_Galaxy S7 Edge',\n",
       "       'is_Galaxy J7 Prime', 'is_Moto G2 3G Dual', 'is_Galaxy S6 Edge',\n",
       "       'is_Galaxy S6 Flat', 'is_5c', 'is_6S Plus', 'is_Galaxy J7',\n",
       "       'is_6 Plus', 'is_Moto G4 Plus', 'is_SE',\n",
       "       'is_Galaxy Gran Prime Duos TV', 'is_4S', 'is_Galaxy S8 Plus',\n",
       "       'is_5', 'is_Moto G3 4G', 'is_Galaxy A5 2017',\n",
       "       'is_Moto X Play 4G Dual', 'is_Moto G5 Plus', 'is_Galaxy A7 2017',\n",
       "       'is_Moto X2', 'is_Galaxy A5', 'is_other_model'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_completo.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creacion del set de training y del de test\n",
    "Vamos a seleccionar solo 980 casos que no realizaron una compra para balancear el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    }
   ],
   "source": [
    "df_completo_reduced_1 = df_completo[df_completo['target']==1]\n",
    "print(len(df_completo_reduced_1.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    }
   ],
   "source": [
    "df_completo_reduced_0 = df_completo[df_completo['target']==0].sample(n=980, random_state=1)\n",
    "print(len(df_completo_reduced_0.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_completo_reduced_1, df_completo_reduced_0])\n",
    "del df_completo_reduced_0, df_completo_reduced_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con aquellas columnas que no sean target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = list(set(df_train.columns.tolist())-set([\"target\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos el dataset de test y lo eliminamos del dataset de train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_train.sample(frac=0.2, random_state=2)\n",
    "df_train.drop(df_test.index,axis=0,inplace=True)\n",
    "train_indexes = df_train.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos el balance de categorías en cada datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = df_train.shape[0]*1.\n",
    "N_test = df_test.shape[0]*1.\n",
    "\n",
    "N_train_1 = df_train.target.sum()\n",
    "N_train_0 = df_train.shape[0]*1.-N_train_1\n",
    "\n",
    "N_test_1 = df_test.target.sum()\n",
    "N_test_0 = df_test.shape[0]*1.-N_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de casos positivos en el train: %50.255102040816325\n",
      "Porcentaje de casos positivos en el test: %48.97959183673469\n"
     ]
    }
   ],
   "source": [
    "print(\"Porcentaje de casos positivos en el train: %\" + str(100.0*N_train_1/N_train))\n",
    "print(\"Porcentaje de casos positivos en el test: %\" + str(100.0*N_test_1/N_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Utilizando únicamente el dataset de train ejecutamos 5-fold Cross Validation, para un modelo de arbol de decision con max_depth igual a 3 y criterion de Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy','roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(max_depth=3, criterion='gini') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n",
    "                        df_train[feature_columns].values, # X_train features\n",
    "                        df_train[\"target\"].values, # Y_train targets\n",
    "                        scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                        return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                        return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                        cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                        n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los resultados en un DF de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5_fold_resultados = pd.DataFrame(scores)\n",
    "cv_5_fold_resultados.drop([\"fit_time\",\"score_time\",\"estimator\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.780255</td>\n",
       "      <td>0.760766</td>\n",
       "      <td>0.801566</td>\n",
       "      <td>0.829917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.762360</td>\n",
       "      <td>0.835382</td>\n",
       "      <td>0.826140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.778309</td>\n",
       "      <td>0.762739</td>\n",
       "      <td>0.842407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.763578</td>\n",
       "      <td>0.766534</td>\n",
       "      <td>0.815797</td>\n",
       "      <td>0.833026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.766773</td>\n",
       "      <td>0.764143</td>\n",
       "      <td>0.812388</td>\n",
       "      <td>0.830726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "0       0.780255        0.760766      0.801566       0.829917\n",
       "1       0.770701        0.762360      0.835382       0.826140\n",
       "2       0.707006        0.778309      0.762739       0.842407\n",
       "3       0.763578        0.766534      0.815797       0.833026\n",
       "4       0.766773        0.764143      0.812388       0.830726"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos los estadisticos de cada experimento, tanto para train como para test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.757663</td>\n",
       "      <td>0.766423</td>\n",
       "      <td>0.805574</td>\n",
       "      <td>0.832443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.029002</td>\n",
       "      <td>0.006982</td>\n",
       "      <td>0.026881</td>\n",
       "      <td>0.006097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.760766</td>\n",
       "      <td>0.762739</td>\n",
       "      <td>0.826140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.763578</td>\n",
       "      <td>0.762360</td>\n",
       "      <td>0.801566</td>\n",
       "      <td>0.829917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.766773</td>\n",
       "      <td>0.764143</td>\n",
       "      <td>0.812388</td>\n",
       "      <td>0.830726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.766534</td>\n",
       "      <td>0.815797</td>\n",
       "      <td>0.833026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.780255</td>\n",
       "      <td>0.778309</td>\n",
       "      <td>0.835382</td>\n",
       "      <td>0.842407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "count       5.000000        5.000000      5.000000       5.000000\n",
       "mean        0.757663        0.766423      0.805574       0.832443\n",
       "std         0.029002        0.006982      0.026881       0.006097\n",
       "min         0.707006        0.760766      0.762739       0.826140\n",
       "25%         0.763578        0.762360      0.801566       0.829917\n",
       "50%         0.766773        0.764143      0.812388       0.830726\n",
       "75%         0.770701        0.766534      0.815797       0.833026\n",
       "max         0.780255        0.778309      0.835382       0.842407"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos el árbol que mejor accuracy haya tenido durante el train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dtree_index = np.argmax(scores[\"test_accuracy\"]*scores[\"test_roc_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_btree = scores[\"estimator\"][best_dtree_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance del modelo sobre test\n",
    "\n",
    "Evaluamos su accuracy sobre el 20% restante del dataset que no formó parte del 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_esperado = df_test.target.values\n",
    "y_pred = best_btree.predict(df_test[feature_columns].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score validation: 0.7882653061224489\n"
     ]
    }
   ],
   "source": [
    "print(\"Score validation: \" + str(accuracy_score(y_esperado, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot_data = StringIO()\n",
    "# export_graphviz(best_btree, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True)\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Evaluamos las mejores combinaciones de depth y metrica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con *get_performance_models* estudiamos el comportamiento del algoritmo *DecisionTree* bajo distintos niveles de profundidad y criterios de selección de features para un dado nodo (Gini o Information Gain)\n",
    "\n",
    "\n",
    " -  criterions = ['gini', 'entropy']\n",
    " \n",
    "-  depths = [3, 6, None]\n",
    "\n",
    "Para tal fin se va a utilizar un procedimiento de 5 fold cross validation. Se informan los resultados de accuracy y auc_roc para validadcion y training de cada fold\n",
    "\n",
    "Por ultimo se selecciona la combinacion de hiprparamentros que tuvo la mejor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_models(dataframe, depths, criterions):\n",
    "    \n",
    "    d = {'comb_hiperparamentros': [], 'test_accuracy': [],  'train_accuracy': [], 'test_roc_auc': [], 'train_roc_auc': []}\n",
    "    \n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    for criterion in criterions:\n",
    "        \n",
    "        for depth in depths:\n",
    "            \n",
    "            dtree = DecisionTreeClassifier(max_depth=depth, criterion=criterion)\n",
    "            \n",
    "            scoring = ['accuracy','roc_auc']\n",
    "    \n",
    "            scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n",
    "                                df_train.drop(columns=['target']).values, # X_train features\n",
    "                                df_train[\"target\"].values, # Y_train targets\n",
    "                                scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                                return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                                return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                                cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                                n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                               )\n",
    "        \n",
    "            cv_5_fold_resultados = pd.DataFrame(scores)\n",
    "            \n",
    "            cv_5_fold_resultados.drop([\"fit_time\",\"score_time\",\"estimator\"],axis=1,inplace=True)\n",
    "                    \n",
    "            cv_5_fold_resultados['comb_hiperparamentros'] = [criterion + '_' + str(depth)]*5\n",
    "            \n",
    "            df = pd.concat([df, cv_5_fold_resultados])\n",
    "    \n",
    "    df.rename(columns={'test_accuracy': 'validation_accuracy', 'test_roc_auc': 'validation_roc_auc'}, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comb_hiperparamentros</th>\n",
       "      <th>validation_accuracy</th>\n",
       "      <th>validation_roc_auc</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gini_3</td>\n",
       "      <td>0.780255</td>\n",
       "      <td>0.801566</td>\n",
       "      <td>0.760766</td>\n",
       "      <td>0.829917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gini_3</td>\n",
       "      <td>0.770701</td>\n",
       "      <td>0.835382</td>\n",
       "      <td>0.762360</td>\n",
       "      <td>0.826140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gini_3</td>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.762739</td>\n",
       "      <td>0.778309</td>\n",
       "      <td>0.842407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gini_3</td>\n",
       "      <td>0.763578</td>\n",
       "      <td>0.815797</td>\n",
       "      <td>0.766534</td>\n",
       "      <td>0.833026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gini_3</td>\n",
       "      <td>0.766773</td>\n",
       "      <td>0.812388</td>\n",
       "      <td>0.764143</td>\n",
       "      <td>0.830726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gini_6</td>\n",
       "      <td>0.748408</td>\n",
       "      <td>0.764809</td>\n",
       "      <td>0.836523</td>\n",
       "      <td>0.908199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gini_6</td>\n",
       "      <td>0.773885</td>\n",
       "      <td>0.809437</td>\n",
       "      <td>0.847687</td>\n",
       "      <td>0.912964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gini_6</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.726935</td>\n",
       "      <td>0.846890</td>\n",
       "      <td>0.905411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gini_6</td>\n",
       "      <td>0.776358</td>\n",
       "      <td>0.789441</td>\n",
       "      <td>0.841434</td>\n",
       "      <td>0.909060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gini_6</td>\n",
       "      <td>0.757188</td>\n",
       "      <td>0.808182</td>\n",
       "      <td>0.830279</td>\n",
       "      <td>0.906678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gini_None</td>\n",
       "      <td>0.697452</td>\n",
       "      <td>0.697501</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gini_None</td>\n",
       "      <td>0.710191</td>\n",
       "      <td>0.710200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gini_None</td>\n",
       "      <td>0.656051</td>\n",
       "      <td>0.656362</td>\n",
       "      <td>0.999203</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gini_None</td>\n",
       "      <td>0.738019</td>\n",
       "      <td>0.740772</td>\n",
       "      <td>0.999203</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gini_None</td>\n",
       "      <td>0.667732</td>\n",
       "      <td>0.667708</td>\n",
       "      <td>0.999203</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entropy_3</td>\n",
       "      <td>0.789809</td>\n",
       "      <td>0.822724</td>\n",
       "      <td>0.759171</td>\n",
       "      <td>0.827495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entropy_3</td>\n",
       "      <td>0.780255</td>\n",
       "      <td>0.843050</td>\n",
       "      <td>0.761563</td>\n",
       "      <td>0.824821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entropy_3</td>\n",
       "      <td>0.707006</td>\n",
       "      <td>0.762739</td>\n",
       "      <td>0.778309</td>\n",
       "      <td>0.842407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entropy_3</td>\n",
       "      <td>0.763578</td>\n",
       "      <td>0.802956</td>\n",
       "      <td>0.766534</td>\n",
       "      <td>0.832098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entropy_3</td>\n",
       "      <td>0.769968</td>\n",
       "      <td>0.816022</td>\n",
       "      <td>0.763347</td>\n",
       "      <td>0.830717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entropy_6</td>\n",
       "      <td>0.773885</td>\n",
       "      <td>0.819073</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.901659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entropy_6</td>\n",
       "      <td>0.773885</td>\n",
       "      <td>0.815401</td>\n",
       "      <td>0.836523</td>\n",
       "      <td>0.907944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entropy_6</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.770286</td>\n",
       "      <td>0.846093</td>\n",
       "      <td>0.921992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entropy_6</td>\n",
       "      <td>0.779553</td>\n",
       "      <td>0.784052</td>\n",
       "      <td>0.826295</td>\n",
       "      <td>0.902913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entropy_6</td>\n",
       "      <td>0.766773</td>\n",
       "      <td>0.820023</td>\n",
       "      <td>0.823904</td>\n",
       "      <td>0.913312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>entropy_None</td>\n",
       "      <td>0.729299</td>\n",
       "      <td>0.729512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entropy_None</td>\n",
       "      <td>0.678344</td>\n",
       "      <td>0.678432</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entropy_None</td>\n",
       "      <td>0.681529</td>\n",
       "      <td>0.686222</td>\n",
       "      <td>0.999203</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entropy_None</td>\n",
       "      <td>0.712460</td>\n",
       "      <td>0.712518</td>\n",
       "      <td>0.999203</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entropy_None</td>\n",
       "      <td>0.706070</td>\n",
       "      <td>0.706149</td>\n",
       "      <td>0.999203</td>\n",
       "      <td>0.999999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comb_hiperparamentros  validation_accuracy  validation_roc_auc  \\\n",
       "0                gini_3             0.780255            0.801566   \n",
       "1                gini_3             0.770701            0.835382   \n",
       "2                gini_3             0.707006            0.762739   \n",
       "3                gini_3             0.763578            0.815797   \n",
       "4                gini_3             0.766773            0.812388   \n",
       "0                gini_6             0.748408            0.764809   \n",
       "1                gini_6             0.773885            0.809437   \n",
       "2                gini_6             0.726115            0.726935   \n",
       "3                gini_6             0.776358            0.789441   \n",
       "4                gini_6             0.757188            0.808182   \n",
       "0             gini_None             0.697452            0.697501   \n",
       "1             gini_None             0.710191            0.710200   \n",
       "2             gini_None             0.656051            0.656362   \n",
       "3             gini_None             0.738019            0.740772   \n",
       "4             gini_None             0.667732            0.667708   \n",
       "0             entropy_3             0.789809            0.822724   \n",
       "1             entropy_3             0.780255            0.843050   \n",
       "2             entropy_3             0.707006            0.762739   \n",
       "3             entropy_3             0.763578            0.802956   \n",
       "4             entropy_3             0.769968            0.816022   \n",
       "0             entropy_6             0.773885            0.819073   \n",
       "1             entropy_6             0.773885            0.815401   \n",
       "2             entropy_6             0.726115            0.770286   \n",
       "3             entropy_6             0.779553            0.784052   \n",
       "4             entropy_6             0.766773            0.820023   \n",
       "0          entropy_None             0.729299            0.729512   \n",
       "1          entropy_None             0.678344            0.678432   \n",
       "2          entropy_None             0.681529            0.686222   \n",
       "3          entropy_None             0.712460            0.712518   \n",
       "4          entropy_None             0.706070            0.706149   \n",
       "\n",
       "   train_accuracy  train_roc_auc  \n",
       "0        0.760766       0.829917  \n",
       "1        0.762360       0.826140  \n",
       "2        0.778309       0.842407  \n",
       "3        0.766534       0.833026  \n",
       "4        0.764143       0.830726  \n",
       "0        0.836523       0.908199  \n",
       "1        0.847687       0.912964  \n",
       "2        0.846890       0.905411  \n",
       "3        0.841434       0.909060  \n",
       "4        0.830279       0.906678  \n",
       "0        1.000000       1.000000  \n",
       "1        1.000000       1.000000  \n",
       "2        0.999203       0.999999  \n",
       "3        0.999203       0.999999  \n",
       "4        0.999203       0.999999  \n",
       "0        0.759171       0.827495  \n",
       "1        0.761563       0.824821  \n",
       "2        0.778309       0.842407  \n",
       "3        0.766534       0.832098  \n",
       "4        0.763347       0.830717  \n",
       "0        0.818182       0.901659  \n",
       "1        0.836523       0.907944  \n",
       "2        0.846093       0.921992  \n",
       "3        0.826295       0.902913  \n",
       "4        0.823904       0.913312  \n",
       "0        1.000000       1.000000  \n",
       "1        1.000000       1.000000  \n",
       "2        0.999203       0.999999  \n",
       "3        0.999203       0.999999  \n",
       "4        0.999203       0.999999  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterions = ['gini', 'entropy']\n",
    "\n",
    "depths = [3, 6, None]\n",
    "\n",
    "df_performance = get_performance_models(df_train, depths, criterions)\n",
    "\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener el modelo que obtuvo la mejor performance promediamos los resultados obtenidos sobre el set de test de cada fold para cada combinacion de hiperparamentros. La mejor combinacion de hiperparamentros es la de Information Gain y altura maxima de 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comb_hiperparamentros</th>\n",
       "      <th>validation_accuracy_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entropy_6</td>\n",
       "      <td>0.764042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comb_hiperparamentros  validation_accuracy_mean\n",
       "1             entropy_6                  0.764042"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'validation_accuracy': ['mean']}\n",
    "\n",
    "df_per = df_performance.groupby(['comb_hiperparamentros']).agg(d)\n",
    "\n",
    "df_per.columns = ['_'.join(col) for col in df_per.columns.values]\n",
    "\n",
    "df_per = df_per.reset_index()\n",
    "\n",
    "df_per[df_per['validation_accuracy_mean']==df_per['validation_accuracy_mean'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Volvemos a correr el 5-fold CV pero completado el dataset de desarollo con un dado porcentaje de completitud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se completan los datos faltantes estableciendo un porcentage entre 0% y 80%, variando de a 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_80 = preprocess(df_final.loc[train_indexes].copy(),\"moda\",.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1568, 28)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.loc[train_indexes].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n",
    "                        df_train_80[feature_columns].values, # X_train features\n",
    "                        df_train_80[\"target\"].values, # Y_train targets\n",
    "                        scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                        return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                        return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                        cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                        n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5_fold_resultados = pd.DataFrame(scores)\n",
    "cv_5_fold_resultados.drop([\"fit_time\",\"score_time\",\"estimator\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.657484</td>\n",
       "      <td>0.815793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.646486</td>\n",
       "      <td>0.824206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.777328</td>\n",
       "      <td>0.687222</td>\n",
       "      <td>0.837719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.732794</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.822053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.740891</td>\n",
       "      <td>0.676111</td>\n",
       "      <td>0.838295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "0       0.682540        0.718367      0.657484       0.815793\n",
       "1       0.661290        0.764228      0.646486       0.824206\n",
       "2       0.622951        0.777328      0.687222       0.837719\n",
       "3       0.688525        0.732794      0.755000       0.822053\n",
       "4       0.622951        0.740891      0.676111       0.838295"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.655651</td>\n",
       "      <td>0.746721</td>\n",
       "      <td>0.684461</td>\n",
       "      <td>0.827613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.031520</td>\n",
       "      <td>0.023849</td>\n",
       "      <td>0.042494</td>\n",
       "      <td>0.009981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.646486</td>\n",
       "      <td>0.815793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.732794</td>\n",
       "      <td>0.657484</td>\n",
       "      <td>0.822053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.740891</td>\n",
       "      <td>0.676111</td>\n",
       "      <td>0.824206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.687222</td>\n",
       "      <td>0.837719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.777328</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.838295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test_accuracy  train_accuracy  test_roc_auc  train_roc_auc\n",
       "count       5.000000        5.000000      5.000000       5.000000\n",
       "mean        0.655651        0.746721      0.684461       0.827613\n",
       "std         0.031520        0.023849      0.042494       0.009981\n",
       "min         0.622951        0.718367      0.646486       0.815793\n",
       "25%         0.622951        0.732794      0.657484       0.822053\n",
       "50%         0.661290        0.740891      0.676111       0.824206\n",
       "75%         0.682540        0.764228      0.687222       0.837719\n",
       "max         0.688525        0.777328      0.755000       0.838295"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_fold_resultados.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dtree_index = np.argmax(scores[\"test_roc_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_btree = scores[\"estimator\"][best_dtree_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_esperado = df_test.target.values\n",
    "y_pred = best_btree.predict(df_test[feature_columns].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score validation: 0.5663265306122449\n"
     ]
    }
   ],
   "source": [
    "print(\"Score validation: \" + str(accuracy_score(y_esperado, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot_data = StringIO()\n",
    "# export_graphviz(best_btree, out_file=dot_data,  \n",
    "#                 filled=True, rounded=True,\n",
    "#                 special_characters=True)\n",
    "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "# Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b y c) Analisis del tamaño del arbol y el accuracy en funcion del porcentaje de datos completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_moda = test_completitud(train=df_final.loc[train_indexes].copy(),\n",
    "                 test=df_test.copy(),\n",
    "                 feature_columns=feature_columns,\n",
    "                 metodo=\"moda\",\n",
    "                 best_depth=6\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_moda_por_clase = test_completitud(train=df_final.loc[train_indexes].copy(),\n",
    "                 test=df_test.copy(),\n",
    "                 feature_columns=feature_columns,\n",
    "                 metodo=\"moda_por_clase\",\n",
    "                 best_depth=6\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En en los dos siguientes graficos se observa como, a medida que aumenta el nivel de completitud de los datos, tanto el tamaño del arbol como el nivel de accuracy mejoran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(test_moda[\"completitud\"],test_moda[\"tamanio\"], linewidth=3.0)\n",
    "ax1.set_xlabel('Porcentaje de completitud')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Cantidad de nodos del arbol')\n",
    "plt.title(\"Imputacion de datos faltantes por moda\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(test_moda[\"completitud\"],test_moda[\"precision\"], linewidth=3.0)\n",
    "ax1.set_xlabel('Porcentaje de completitud')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Accuracy')\n",
    "plt.title(\"Imputacion de datos faltantes por moda\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En en los dos siguientes graficos se observa un comportamiento similar a los graficos anteriores. Es decir, a medida que aumenta el nivel de completitud de los datos, tanto el tamaño del arbol como el nivel de accuracy mejoran.\n",
    "\n",
    "Tambien podemos observar que la accuracy sobre el set de test es mucho menor, producto de haber overfiteado el modelo al completar datos faltantes con la moda de clase. Esto se produce porque al hacer eso estamos \"filtrando\" informacion del test de validacion en el test de training, lo que permite obtener muy buenos resultados en el set de desarrollo. Sin embargo, cuando intentamos predecir con el modelo sobre observaciones que no tienen esas caracteristicas los resultados empeoran notablemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(test_moda_por_clase[\"completitud\"],test_moda_por_clase[\"tamanio\"], linewidth=3.0)\n",
    "ax1.set_xlabel('Porcentaje de completitud')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Cantidad de nodos del arbol')\n",
    "plt.title(\"Imputacion de datos faltantes por moda de clase\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(test_moda_por_clase[\"completitud\"],test_moda_por_clase[\"precision\"], linewidth=3.0)\n",
    "ax1.set_xlabel('Porcentaje de completitud')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Accuracy')\n",
    "plt.title(\"Imputacion de datos faltantes por moda de clase\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Tolerancia al ruido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) y b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generacion de ruido \n",
    "Creamos una funcion que genera un array de un largo determinado con valores 0 y un porcentage de valores random. Los valores random que se generen van a depender de la media de la columna para la cual quiere generarse ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array_noise(elements_in_column, percentage_noise, mean_column):\n",
    "    \n",
    "    number_noise = round(elements_in_column*percentage_noise)\n",
    "    \n",
    "    mu, sigma = 0, mean_column\n",
    "    \n",
    "    noise = np.random.normal(mu, sigma, [number_noise,1])\n",
    "    \n",
    "    array_noise = noise.tolist()\n",
    "    \n",
    "    array_noise_final = []\n",
    "    \n",
    "    for item in array_noise:\n",
    "        \n",
    "        array_noise_final.append(item[0])\n",
    "    \n",
    "    array_no_noise = [0]*(elements_in_column - number_noise)\n",
    "    \n",
    "    noise = array_noise_final + array_no_noise\n",
    "    \n",
    "    random.shuffle(noise)\n",
    "    \n",
    "    return noise    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funcion que, utilizando la funcion anterior, agrega ruido a un % dado de valores del atributo que indiquemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_with_noise(df, name_attribute, percentage_noise):\n",
    "    \n",
    "    dataframe = df.copy()\n",
    "    \n",
    "    number_observations = len(dataframe[name_attribute].values)\n",
    "    \n",
    "    mean_column_attribute = dataframe[name_attribute].mean()\n",
    "    \n",
    "    array_noise = create_array_noise(number_observations, percentage_noise, mean_column_attribute)\n",
    "    \n",
    "    dataframe[name_attribute] = dataframe[name_attribute] + array_noise\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funcion que recibe como inputs un dataset de train, otro de test y el nombre de un atributo, que sera sobre el cual se va a generar ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_familiy_datasets(df_train, df_test, name_attribute):\n",
    "    \n",
    "    # Modelo con la mejor combinacion de hiperparametros del punto 2.2\n",
    "    \n",
    "    dtree = DecisionTreeClassifier(max_depth=6, criterion='gini')\n",
    "    \n",
    "    noise_accuracy_tamanio = {\"noise\":[], \"accuracy\":[], \"tamanio\":[]}\n",
    "    \n",
    "    # Niveles de ruido\n",
    "    \n",
    "    for noise in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35]:\n",
    "        \n",
    "        # Agregamos el ruido al attribute elegido\n",
    "        \n",
    "        df_noise = generate_dataset_with_noise(df_train, name_attribute, noise)\n",
    "        \n",
    "        scoring = ['accuracy']\n",
    "        \n",
    "        scores = cross_validate(dtree, # Instancia de árbol a entrenar en cada fold\n",
    "                            df_noise.drop(columns=['target']).values, # X_train features\n",
    "                            df_noise[\"target\"].values, # Y_train targets\n",
    "                            scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                            return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                            return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                            cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                            n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                           )\n",
    "        \n",
    "        # Seleccionamos el mejor modelo\n",
    "        \n",
    "        best_dtree_index = np.argmax(scores[\"test_accuracy\"])\n",
    "        best_btree = scores[\"estimator\"][best_dtree_index]\n",
    "\n",
    "        y_esperado = df_test['target'].values\n",
    "        y_pred = best_btree.predict(df_test.drop(columns=['target']).values)\n",
    "\n",
    "        accuracy = accuracy_score(y_esperado, y_pred)\n",
    "        tamanio = best_btree.tree_.node_count\n",
    "\n",
    "        noise_accuracy_tamanio[\"noise\"].append(100.0*noise)\n",
    "        noise_accuracy_tamanio[\"accuracy\"].append(accuracy)\n",
    "        noise_accuracy_tamanio[\"tamanio\"].append(tamanio)\n",
    "        \n",
    "    return noise_accuracy_tamanio\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a seleccionar el atributo que mas 'importancia' tiene en el modelo ('checkout') y el que menos tiene ('Android') para generarles ruido y observar los resultados. La motivacion es analizar que diferencias arrojan los resultados en ambos casos.\n",
    "\n",
    "#### Nota: Los modelos de arboles de decision de sklearn tienen un atributo  'feature_importances_'. De acuerdo a la documentacion de esta libreria, lo que hace 'feature_importances_' es asignar un coefiente de importancia mayor cuando mas cerca esta la feature de la raiz del arbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(criterion='entropy', max_depth=6)\n",
    "\n",
    "dtree.fit(df_train.drop(columns=['target']), df_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = dtree.feature_importances_\n",
    "\n",
    "maxposition = feature_importances.tolist().index(max(feature_importances))\n",
    "\n",
    "df_train.drop(columns=['target']).columns.values[maxposition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minposition = feature_importances.tolist().index(min(feature_importances))\n",
    "\n",
    "df_train.columns.values[minposition]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)\n",
    "\n",
    "#### Lo que se observa en los resultados es que la accuracy no se ve afectada al agregar ruido al atributo 'checkout' ni a 'Android', a pesar de que la primera es la feature mas 'importante' y la segunda la menos. Pareciese que los resultados del arbol son robustos al ruido. Sin embargo, algo que si podemos mencionar es que la variabilidad del accuracy obtenido para los diferentes niveles de noise es mas alta en el caso de la variable 'checkout' que en el de 'Android'.\n",
    "\n",
    "#### Lo que si se observa es un aumento del tamaño en el arbol a medida que la cantidad de ruido aumenta, en cualquiera de los ods atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_2_4_checkout = run_model_familiy_datasets(df_train, df_test, 'checkout')\n",
    "\n",
    "pd.DataFrame(data=df_results_2_4_checkout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(pd.DataFrame(data=df_results_2_4_checkout)['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_2_4_min = run_model_familiy_datasets(df_train, df_test, 'Android')\n",
    "\n",
    "pd.DataFrame(data=df_results_2_4_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(pd.DataFrame(data=df_results_2_4_min)['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Graficamos el tamaño del arbol (cantidad de nodos) en funcion del ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(df_results_2_4_checkout[\"noise\"], df_results_2_4_checkout[\"tamanio\"], linewidth=3.0)\n",
    "ax1.set_xlabel('Porcentaje de noise')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Cantidad de nodos del arbol')\n",
    "plt.title(\"Relacion noise y tamaño del arbol - Checkout\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Graficamos la performance (accuracy) en funcion del ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(df_results_2_4_checkout[\"noise\"], df_results_2_4_checkout[\"accuracy\"], linewidth=3.0)\n",
    "ax1.set_xlabel('Porcentaje de noise')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('Accuracy')\n",
    "plt.title(\"Relacion noise y accuracy - Checkout\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el modelo con los datos de train y predecimos sobre test. Los resultamos que vamos a obtener son las probabilidades que el modelo asigna para cada clase posible (recordemos que en este caso son solo 2: realizo una compra entre el 01/06/2018 y el 15/06/2018 o no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "\n",
    "scoring = ['accuracy','roc_auc']\n",
    "\n",
    "scores_nb = cross_validate(clf, # Instancia de árbol a entrenar en cada fold\n",
    "                        df_train.drop(columns=['target']).values, # X_train features\n",
    "                        df_train[\"target\"].values, # Y_train targets\n",
    "                        scoring=scoring, # Pedimos que para fold se ejecute esta lista de scorings\n",
    "                        return_train_score = True, # Queremos ver los scorings de los training folds\n",
    "                        return_estimator = True, # Ademas, pedimos que se nos devuelvan los dtrees de entrenados\n",
    "                        cv=5, # Establecemos la cantidad de particiones para realizar la Validación cruzada\n",
    "                        n_jobs = -1 # Pedimos que se ejecute en paralelo, utilizando todos los cores\n",
    "                       )\n",
    "scores_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5_fold_resultados_naive_bayes = pd.DataFrame(scores_nb)\n",
    "\n",
    "cv_5_fold_resultados_naive_bayes.drop([\"fit_time\",\"score_time\",\"estimator\"],axis=1,inplace=True)\n",
    "\n",
    "cv_5_fold_resultados_naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_5_fold_resultados_naive_bayes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_naive_bayes_index = np.argmax(scores_nb[\"test_accuracy\"]*scores_nb[\"test_roc_auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_naive_bayes = scores[\"estimator\"][best_naive_bayes_index]\n",
    "\n",
    "y_esperado_nb = df_test.target.values\n",
    "\n",
    "y_pred_nb = best_naive_bayes.predict(df_test[feature_columns].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy del modelo naive bayes sobre el dataset de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score validation: \" + str(accuracy_score(y_esperado_nb, y_pred_nb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenemos las probabilidades de pertenecia a cada clase que el modelo calcula para cada observacion de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_probabilities(model, dataset):\n",
    "    \n",
    "    y_pred = model.predict_proba(dataset.drop(columns=['target']))\n",
    "    \n",
    "    array_pred_class_0 = []\n",
    "    \n",
    "    array_pred_class_1 = []\n",
    "    \n",
    "    for prediction in y_pred:\n",
    "        \n",
    "        array_pred_class_0.append('{:f}'.format(prediction[0]))\n",
    "        \n",
    "        array_pred_class_1.append('{:f}'.format(prediction[1]))\n",
    "    \n",
    "    d = {'proba_class_0': array_pred_class_0, 'proba_class_1': array_pred_class_1}\n",
    "    \n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    return df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_probabilities(best_naive_bayes, df_test).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenemos las probabilidades condicionales para un atributo del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditional_probabilities_attribute(dataframe, name_attribute):\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    df['target_no'] = np.where(df['target'] == 0, 1, 0)\n",
    "    \n",
    "    d = {'target': ['sum'], 'target_no': ['sum']}\n",
    "\n",
    "    df_cond = df.groupby([name_attribute]).agg(d)\n",
    "\n",
    "    df_cond.columns = ['_'.join(col) for col in df_cond.columns.values]\n",
    "\n",
    "    df_cond = df_cond.reset_index()\n",
    "\n",
    "    df_cond['target_yes'] = round(df_cond['target_sum']/df_cond['target_sum'].sum(),3)\n",
    "    \n",
    "    df_cond['target_no'] = round(df_cond['target_no_sum']/df_cond['target_no_sum'].sum(),3)\n",
    "    \n",
    "    df_cond = df_cond.drop(columns=['target_sum', 'target_no_sum'])\n",
    "    \n",
    "    return df_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Por ejemplo, dado el valor de 1 para target, la probabilidad de que 'Android' tome el valor de 0 es de 0,594 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_cond_proba = get_conditional_probabilities_attribute(df_train, 'Android')\n",
    "\n",
    "print(df_test_cond_proba.head())\n",
    "\n",
    "df_test_cond_proba['target_yes'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "### Naive Bayes vs Arbol de decision\n",
    "\n",
    "Vamos a utilizar GridSearch para obtener los hiperparametros que maximizan la performance del modelo de arbol de decision. En el caso de Naive Bayes eso no lo hacemos porque el modelo no tiene hiperparametros que optimizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth':[3,5,6,7,9],\n",
    "               'criterion': ['gini', 'entropy'],\n",
    "              'min_samples_leaf': [3,6,9]\n",
    "             }\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "dtree_best = GridSearchCV(cv=5, n_jobs=-1, estimator=dtree, param_grid = param_grid, scoring='roc_auc')\n",
    "\n",
    "dtree_best = dtree_best.fit(df_train.drop(columns='target'), df_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtree_best.best_score_)\n",
    "print(dtree_best.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = dtree_best.predict(df_test.drop(columns=['target']))\n",
    "\n",
    "print(\"Roc_auc decisition tree: \" + str(roc_auc_score(df_test['target'], y_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Roc_auc naive bayes: \" + str(roc_auc_score(y_esperado_nb, y_pred_nb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "\n",
    "Se observa que el modelo que mejor performance tiene es el de arbol de decision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
